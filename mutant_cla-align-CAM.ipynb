{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "import random\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pysgmcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def read_mutant_txt(path):\n",
    "    name_list = []\n",
    "    fo = open(path)\n",
    "    for line in fo:\n",
    "        striped_line = line.strip('\\n')\n",
    "        if striped_line != '':\n",
    "            name_list.append(striped_line)\n",
    "    return name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutant_names = read_mutant_txt('mutant_imgs.txt')\n",
    "data_base_path = '/scratch/zq415/grammar_cor/mutant_detect/data'\n",
    "data_folder_list = ['20180419_newdata_nii_with_filtered', 'new_data_20180522_nii', 'organized_data_nii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'All_data_112_64_64.pickle'\n",
    "with open(os.path.join(os.getcwd(),'data',save_name), \"rb\") as input_file:\n",
    "    all_train_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ['20170207_En1_E13_E3_Mut', '20170207_En1_E13_E3c_Mut', '20170207_En1_E13_E3d_Mut', '20161206_En1_E13_E3a', '20161206_En1_E13_E3b', '20161206_En1_E13_E3c', '20161206_En1_E13_E3d', '20180131_En1_E12_E5a_Mut', '20180131_En1_E12_E5b', '20180131_En1_E12_E5c_Mut', '20180202_En1_E14_E3a_Mut', '20180202_En1_E14_E3b_Mut', '20171211_En1_E10_E1a', '20170619_En1_E12_E3a', '20170706_En1_E13_E11a_Mut_reg', '20171009_En1_E12_E10a', '20171211_En1_E13_E5a_Mut_Ext', '20180201_En1_M2_E13_E4a']\n"
     ]
    }
   ],
   "source": [
    "mutant_group = [(9,10), (12,13,14), (16,17,18,19), (36,37), (38,39), (42,43), (44,45,46,47), (48,49,50,51), (52,53), (54,55),\n",
    "(56,57,58), (59,60,61), (63,64), (66,67,68), (69,70,71), (73,74), (75,76), (77,78,79), (80,81,82,83,84,85,86,87),\n",
    "(89,90), (91,92), (93,94), (95,96,97), (98,99), (100,101,102)]\n",
    "\n",
    "group_list = []\n",
    "for one_group in mutant_group:\n",
    "    for ii in range(len(one_group)):\n",
    "        group_list.append(one_group[ii])\n",
    "single_mutant = [i for i in range(len(mutant_names)) if i not in group_list]\n",
    "\n",
    "test_mut_names = []\n",
    "for i in range(1,len(mutant_group),6):\n",
    "    for ii in range(len(mutant_group[i])):\n",
    "        test_mut_names.append(mutant_names[mutant_group[i][ii]])\n",
    "\n",
    "for i in range(1,len(single_mutant),6):\n",
    "    test_mut_names.append(mutant_names[single_mutant[i]])\n",
    "    \n",
    "print(len(test_mut_names),test_mut_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(all_train_data)):\n",
    "    if all_train_data[i][2] in mutant_names:\n",
    "        if all_train_data[i][2] in test_mut_names:\n",
    "            test_data.append((all_train_data[i][3]-0.5, 0 ))\n",
    "        else:\n",
    "            train_data.append((all_train_data[i][3]-0.5,0))\n",
    "    else:\n",
    "        random.seed(i*8)\n",
    "        if random.uniform(0,1) < 0.16:\n",
    "            test_data.append((all_train_data[i][3]-0.5,1))\n",
    "        else:\n",
    "            train_data.append((all_train_data[i][3]-0.5,1))\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_nft = nib.Nifti1Image(np.squeeze(all_train_data[193]),np.eye(4))\n",
    "#img_save_data_path = './img/mul_img.nii'\n",
    "#nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Mouse_sub_volumes(Dataset):\n",
    "    \"\"\"Mouse sub-volumes BV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, all_data , transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            all_whole_volumes: Contain all the padded whole BV volumes as a dic\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.all_data = all_data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, num):\n",
    "        \n",
    "        current_img, label = self.all_data[num]\n",
    "        \n",
    "        img = np.float32(current_img[np.newaxis,...])\n",
    "        sample = {'image': img, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Flip the image for data augmentation, but prefer original image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,ori_probability=0.20):\n",
    "        self.ori_probability = ori_probability\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0,1) < self.ori_probability:\n",
    "            return sample\n",
    "        else:\n",
    "            img, label = sample['image'], sample['label']\n",
    "            random_choise1=random.choice([1,2,3,4,5,6,7,8])\n",
    "            img[0,...] = {1: lambda x: x,\n",
    "                          2: lambda x: x[::-1,:,:],\n",
    "                          3: lambda x: x[:,::-1,:],\n",
    "                          4: lambda x: x[:,:,::-1],\n",
    "                          5: lambda x: x[::-1,::-1,:],\n",
    "                          6: lambda x: x[::-1,:,::-1],\n",
    "                          7: lambda x: x[:,::-1,::-1],\n",
    "                          8: lambda x: x[::-1,::-1,::-1]\n",
    "                          }[random_choise1](img[0,...])\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class VGG_net(nn.Module):\n",
    "    def __init__(self,conv_drop_rate=0.10,linear_drop_rate=0.2):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=12, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv1_bn = nn.BatchNorm3d(12)\n",
    "        self.conv2 = nn.Conv3d(in_channels=12, out_channels=12, kernel_size=3,stride=1,padding=2, dilation=2)\n",
    "        self.conv2_bn = nn.BatchNorm3d(12)\n",
    "        self.pool1 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout1 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(in_channels=12, out_channels=24, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv3_bn = nn.BatchNorm3d(24)\n",
    "        self.conv4 = nn.Conv3d(in_channels=24, out_channels=24, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv4_bn = nn.BatchNorm3d(24)\n",
    "        self.pool2 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout2 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv5 = nn.Conv3d(in_channels=24, out_channels=48, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv5_bn = nn.BatchNorm3d(48)\n",
    "        self.conv6 = nn.Conv3d(in_channels=48, out_channels=48, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv6_bn = nn.BatchNorm3d(48)\n",
    "        self.pool3 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout3 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv7 = nn.Conv3d(in_channels=48, out_channels=96, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv7_bn = nn.BatchNorm3d(96)\n",
    "        self.conv8 = nn.Conv3d(in_channels=96, out_channels=96, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.conv8_bn = nn.BatchNorm3d(96)\n",
    "        self.pool4 = nn.AdaptiveAvgPool3d((1,1,1))\n",
    "        self.dropout4 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1_bn(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(self.pool1(self.conv2_bn(F.relu(self.conv2(x)))))\n",
    "        \n",
    "        x = self.conv3_bn(F.relu(self.conv3(x)))\n",
    "        x = self.dropout2(self.pool2(self.conv4_bn(F.relu(self.conv4(x)))))\n",
    "        \n",
    "        x = self.conv5_bn(F.relu(self.conv5(x)))\n",
    "        x = self.dropout3(self.pool3(self.conv6_bn(F.relu(self.conv6(x)))))\n",
    "        \n",
    "        x = self.conv7_bn(F.relu(self.conv7(x)))\n",
    "        x = self.dropout4(self.pool4(self.conv8_bn(F.relu(self.conv8(x)))))\n",
    "        \n",
    "        x = x.view(-1, 96)\n",
    "        #x = self.dropout5(self.fc1_bn(F.relu(self.fc1(x))))\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/MFreidank/pysgmcmc@pytorch\n",
      "  Cloning https://github.com/MFreidank/pysgmcmc (to revision pytorch) to /state/partition1/job-5988547/pip-req-build-_6j_xeqz\n",
      "Branch pytorch set up to track remote branch pytorch from origin.\n",
      "Switched to a new branch 'pytorch'\n",
      "Requirement already satisfied (use --upgrade to upgrade): pysgmcmc==0.0.1 from git+https://github.com/MFreidank/pysgmcmc@pytorch in /home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: setuptools>=35.0.2 in /home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages (from pysgmcmc==0.0.1) (40.6.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from pysgmcmc==0.0.1) (1.13.3)\n",
      "Requirement already satisfied: torchvision in /home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages (from pysgmcmc==0.0.1) (0.2.1)\n",
      "Requirement already satisfied: torch in /home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages (from torchvision->pysgmcmc==0.0.1) (0.4.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages (from torchvision->pysgmcmc==0.0.1) (5.3.0)\n",
      "Requirement already satisfied: six in /home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages (from torchvision->pysgmcmc==0.0.1) (1.11.0)\n",
      "Building wheels for collected packages: pysgmcmc\n",
      "  Running setup.py bdist_wheel for pysgmcmc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /state/partition1/job-5988547/pip-ephem-wheel-cache-g_b2tfz4/wheels/0d/08/b8/56459f78d37ad065ad2774ce2b8839bbda57e519e468fc3ed4\n",
      "Successfully built pysgmcmc\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/MFreidank/pysgmcmc@pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysgmcmc.optimizers.sgld import SGLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_net()\n",
    "#net.apply(weight_init)\n",
    "\n",
    "optimizer = SGLD(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        i_batch += 1\n",
    "        if i_batch % 10 == 0:\n",
    "            print(\"epoch {}, batch {}, current loss {}\".format(epoch+1,i_batch,running_loss/10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    \n",
    "    true_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "            inputs = inputs.to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_predicted_labels.append((labels.numpy(), predicted.cpu().numpy()))\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    return true_predicted_labels\n",
    "\n",
    "def get_confusion_matrix(true_predicted_labels):\n",
    "    cross_table = np.zeros([2,2])\n",
    "    mut_to_nor = []\n",
    "    nor_to_mul = []\n",
    "    test_dic = true_predicted_labels\n",
    "    for i in range(len(test_dic)):\n",
    "        if test_dic[i][0] ==0 and test_dic[i][1] ==0:\n",
    "            cross_table[0,0] += 1\n",
    "        elif  test_dic[i][0] ==0 and test_dic[i][1] ==1:\n",
    "            cross_table[0,1] += 1\n",
    "            mut_to_nor.append(i)\n",
    "        elif test_dic[i][0] ==1 and test_dic[i][1] ==0:\n",
    "            cross_table[1,0] += 1\n",
    "            nor_to_mul.append(i)\n",
    "        elif test_dic[i][0] ==1 and test_dic[i][1] ==1:\n",
    "            cross_table[1,1] += 1\n",
    "    print(cross_table)\n",
    "    print(mut_to_nor)\n",
    "    print(nor_to_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 495374 parameters in the model\n",
      "choose SGD as optimizer\n",
      "epoch 1, batch 10, current loss 0.6956422030925751\n",
      "epoch 1, batch 20, current loss 0.6869348227977753\n",
      "epoch 1, batch 30, current loss 0.6631369709968566\n",
      "epoch 2, batch 10, current loss 0.6257618606090546\n",
      "epoch 2, batch 20, current loss 0.6146553337574006\n",
      "epoch 2, batch 30, current loss 0.633811867237091\n",
      "epoch 3, batch 10, current loss 0.5761816740036011\n",
      "epoch 3, batch 20, current loss 0.46665129661560056\n",
      "epoch 3, batch 30, current loss 0.5657233476638794\n",
      "epoch 4, batch 10, current loss 0.4539719194173813\n",
      "epoch 4, batch 20, current loss 0.39927015602588656\n",
      "epoch 4, batch 30, current loss 0.5526776939630509\n",
      "epoch 5, batch 10, current loss 0.41829574182629586\n",
      "epoch 5, batch 20, current loss 0.5022016152739525\n",
      "epoch 5, batch 30, current loss 0.3793440401554108\n",
      "epoch 6, batch 10, current loss 0.29261579662561416\n",
      "epoch 6, batch 20, current loss 0.30306785106658934\n",
      "epoch 6, batch 30, current loss 0.22168605253100396\n",
      "epoch 7, batch 10, current loss 0.19274858795106412\n",
      "epoch 7, batch 20, current loss 0.45873135328292847\n",
      "epoch 7, batch 30, current loss 0.263053822517395\n",
      "epoch 8, batch 10, current loss 0.27842531688511374\n",
      "epoch 8, batch 20, current loss 0.33256359249353407\n",
      "epoch 8, batch 30, current loss 0.27576992511749265\n",
      "epoch 9, batch 10, current loss 0.2027009978890419\n",
      "epoch 9, batch 20, current loss 0.12369090765714645\n",
      "epoch 9, batch 30, current loss 0.23921347111463548\n",
      "epoch 10, batch 10, current loss 0.1403921702876687\n",
      "epoch 10, batch 20, current loss 0.21575535610318183\n",
      "epoch 10, batch 30, current loss 0.27241136208176614\n",
      "epoch 10 train accuracy: \n",
      "total_num:471, test accuracy:0.9490445859872612, positive_acc:0.9428571428571428, negative_acc:0.9767441860465116\n",
      "[[  84.    2.]\n",
      " [  22.  363.]]\n",
      "[180, 193]\n",
      "[12, 38, 45, 50, 75, 80, 101, 135, 143, 163, 192, 195, 196, 220, 233, 237, 310, 321, 352, 357, 454, 462]\n",
      "-------------------\n",
      "epoch 10 test accuracy: \n",
      "total_num:96, test accuracy:0.8958333333333334, positive_acc:0.8974358974358975, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  8.  70.]]\n",
      "[0, 90]\n",
      "[12, 16, 32, 37, 38, 45, 57, 67]\n",
      "epoch 11, batch 10, current loss 0.15188689082860946\n",
      "epoch 11, batch 20, current loss 0.2105055868625641\n",
      "epoch 11, batch 30, current loss 0.17893182151019574\n",
      "epoch 12, batch 10, current loss 0.2873472187668085\n",
      "epoch 12, batch 20, current loss 0.16499734558165075\n",
      "epoch 12, batch 30, current loss 0.15833706445991994\n",
      "epoch 13, batch 10, current loss 0.13231298364698887\n",
      "epoch 13, batch 20, current loss 0.20137509219348432\n",
      "epoch 13, batch 30, current loss 0.20987628325819968\n",
      "epoch 14, batch 10, current loss 0.19486552122980355\n",
      "epoch 14, batch 20, current loss 0.12027559541165829\n",
      "epoch 14, batch 30, current loss 0.14751056917011737\n",
      "epoch 15, batch 10, current loss 0.16469221422448754\n",
      "epoch 15, batch 20, current loss 0.1373836192302406\n",
      "epoch 15, batch 30, current loss 0.10379312559962273\n",
      "epoch 16, batch 10, current loss 0.1926373217254877\n",
      "epoch 16, batch 20, current loss 0.11379808932542801\n",
      "epoch 16, batch 30, current loss 0.14841594994068147\n",
      "epoch 17, batch 10, current loss 0.08557734591886401\n",
      "epoch 17, batch 20, current loss 0.08221567813307047\n",
      "epoch 17, batch 30, current loss 0.03954236963763833\n",
      "epoch 18, batch 10, current loss 0.0768624891526997\n",
      "epoch 18, batch 20, current loss 0.11438742708414792\n",
      "epoch 18, batch 30, current loss 0.10297865951433778\n",
      "epoch 19, batch 10, current loss 0.08910095393657684\n",
      "epoch 19, batch 20, current loss 0.04815261326730251\n",
      "epoch 19, batch 30, current loss 0.1629452715627849\n",
      "epoch 20, batch 10, current loss 0.09700991529971362\n",
      "epoch 20, batch 20, current loss 0.13332931622862815\n",
      "epoch 20, batch 30, current loss 0.0824965400621295\n",
      "epoch 20 train accuracy: \n",
      "total_num:471, test accuracy:0.9808917197452229, positive_acc:0.9818181818181818, negative_acc:0.9767441860465116\n",
      "[[  84.    2.]\n",
      " [   7.  378.]]\n",
      "[180, 193]\n",
      "[71, 101, 163, 192, 220, 352, 454]\n",
      "-------------------\n",
      "epoch 20 test accuracy: \n",
      "total_num:96, test accuracy:0.96875, positive_acc:0.9871794871794872, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  1.  77.]]\n",
      "[0, 90]\n",
      "[57]\n",
      "epoch 21, batch 10, current loss 0.1467569986358285\n",
      "epoch 21, batch 20, current loss 0.12116598486900329\n",
      "epoch 21, batch 30, current loss 0.13574388935230672\n",
      "epoch 22, batch 10, current loss 0.07190919015556574\n",
      "epoch 22, batch 20, current loss 0.10772421443834901\n",
      "epoch 22, batch 30, current loss 0.03503476686310023\n",
      "epoch 23, batch 10, current loss 0.12413049638271331\n",
      "epoch 23, batch 20, current loss 0.14197702053934336\n",
      "epoch 23, batch 30, current loss 0.06103553269058466\n",
      "epoch 24, batch 10, current loss 0.03264224659651518\n",
      "epoch 24, batch 20, current loss 0.06183889638632536\n",
      "epoch 24, batch 30, current loss 0.05289915311150253\n",
      "epoch 25, batch 10, current loss 0.04157839464023709\n",
      "epoch 25, batch 20, current loss 0.04799469213467091\n",
      "epoch 25, batch 30, current loss 0.04215185667271726\n",
      "epoch 26, batch 10, current loss 0.02510470231063664\n",
      "epoch 26, batch 20, current loss 0.04493340775370598\n",
      "epoch 26, batch 30, current loss 0.0645115819759667\n",
      "epoch 27, batch 10, current loss 0.025992420432157815\n",
      "epoch 27, batch 20, current loss 0.0719805764267221\n",
      "epoch 27, batch 30, current loss 0.031941412831656635\n",
      "epoch 28, batch 10, current loss 0.0350384373916313\n",
      "epoch 28, batch 20, current loss 0.07596516669727862\n",
      "epoch 28, batch 30, current loss 0.10300983068300411\n",
      "epoch 29, batch 10, current loss 0.04609671481885016\n",
      "epoch 29, batch 20, current loss 0.1090904769487679\n",
      "epoch 29, batch 30, current loss 0.03560271263122559\n",
      "epoch 30, batch 10, current loss 0.11074799364432693\n",
      "epoch 30, batch 20, current loss 0.09569220067933201\n",
      "epoch 30, batch 30, current loss 0.06422769385389984\n",
      "epoch 30 train accuracy: \n",
      "total_num:471, test accuracy:0.9978768577494692, positive_acc:0.9974025974025974, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   1.  384.]]\n",
      "[]\n",
      "[163]\n",
      "-------------------\n",
      "epoch 30 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9871794871794872, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  1.  77.]]\n",
      "[0, 39, 90]\n",
      "[37]\n",
      "epoch 31, batch 10, current loss 0.026945976098068057\n",
      "epoch 31, batch 20, current loss 0.04421721617691219\n",
      "epoch 31, batch 30, current loss 0.04755061473697424\n",
      "epoch 32, batch 10, current loss 0.055160741019062695\n",
      "epoch 32, batch 20, current loss 0.08903324836865067\n",
      "epoch 32, batch 30, current loss 0.07932214364409447\n",
      "epoch 33, batch 10, current loss 0.07385052237659692\n",
      "epoch 33, batch 20, current loss 0.10820350886788219\n",
      "epoch 33, batch 30, current loss 0.07362121101468802\n",
      "epoch 34, batch 10, current loss 0.03885164593812078\n",
      "epoch 34, batch 20, current loss 0.04440596578642726\n",
      "epoch 34, batch 30, current loss 0.05158058577217162\n",
      "epoch 35, batch 10, current loss 0.01817539589246735\n",
      "epoch 35, batch 20, current loss 0.019813143159262837\n",
      "epoch 35, batch 30, current loss 0.07290770895779133\n",
      "epoch 36, batch 10, current loss 0.03718927474692464\n",
      "epoch 36, batch 20, current loss 0.05735817707609385\n",
      "epoch 36, batch 30, current loss 0.034337665676139294\n",
      "epoch 37, batch 10, current loss 0.048139593773521484\n",
      "epoch 37, batch 20, current loss 0.025088509311899543\n",
      "epoch 37, batch 30, current loss 0.006854089873377234\n",
      "epoch 38, batch 10, current loss 0.028782014542957767\n",
      "epoch 38, batch 20, current loss 0.0810829157475382\n",
      "epoch 38, batch 30, current loss 0.09143826579675078\n",
      "epoch 39, batch 10, current loss 0.03653080314397812\n",
      "epoch 39, batch 20, current loss 0.04613856396172196\n",
      "epoch 39, batch 30, current loss 0.025015425682067872\n",
      "epoch 40, batch 10, current loss 0.014191629423294216\n",
      "epoch 40, batch 20, current loss 0.03706237471196801\n",
      "epoch 40, batch 30, current loss 0.0283036210690625\n",
      "epoch 40 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 40 test accuracy: \n",
      "total_num:96, test accuracy:0.9791666666666666, positive_acc:1.0, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  0.  78.]]\n",
      "[0, 90]\n",
      "[]\n",
      "epoch 41, batch 10, current loss 0.03151481007225811\n",
      "epoch 41, batch 20, current loss 0.030335034476593137\n",
      "epoch 41, batch 30, current loss 0.05599024429393466\n",
      "epoch 42, batch 10, current loss 0.018865654326509683\n",
      "epoch 42, batch 20, current loss 0.018277950258925556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42, batch 30, current loss 0.006617568311048672\n",
      "epoch 43, batch 10, current loss 0.005553472292376682\n",
      "epoch 43, batch 20, current loss 0.006391102919587865\n",
      "epoch 43, batch 30, current loss 0.010187715862412006\n",
      "epoch 44, batch 10, current loss 0.01822757933405228\n",
      "epoch 44, batch 20, current loss 0.01822001617401838\n",
      "epoch 44, batch 30, current loss 0.028836075065191834\n",
      "epoch 45, batch 10, current loss 0.00590320301707834\n",
      "epoch 45, batch 20, current loss 0.013996994990156964\n",
      "epoch 45, batch 30, current loss 0.020867846440523863\n",
      "epoch 46, batch 10, current loss 0.005766959686297923\n",
      "epoch 46, batch 20, current loss 0.03715946272131987\n",
      "epoch 46, batch 30, current loss 0.013497328619996552\n",
      "epoch 47, batch 10, current loss 0.01804182767518796\n",
      "epoch 47, batch 20, current loss 0.01943927933316445\n",
      "epoch 47, batch 30, current loss 0.0051861820160411295\n",
      "epoch 48, batch 10, current loss 0.012055339472135529\n",
      "epoch 48, batch 20, current loss 0.03538017862010747\n",
      "epoch 48, batch 30, current loss 0.024185221816878766\n",
      "epoch 49, batch 10, current loss 0.007371126429643482\n",
      "epoch 49, batch 20, current loss 0.02254371887538582\n",
      "epoch 49, batch 30, current loss 0.009747081552632153\n",
      "epoch 50, batch 10, current loss 0.005544361949432641\n",
      "epoch 50, batch 20, current loss 0.017390784711460584\n",
      "epoch 50, batch 30, current loss 0.049284445095690896\n",
      "epoch 50 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 50 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9743589743589743, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  2.  76.]]\n",
      "[0, 78, 90]\n",
      "[32, 38]\n",
      "epoch 51, batch 10, current loss 0.0048706664820201695\n",
      "epoch 51, batch 20, current loss 0.013634754589293151\n",
      "epoch 51, batch 30, current loss 0.03678723920602352\n",
      "epoch 52, batch 10, current loss 0.002586146190878935\n",
      "epoch 52, batch 20, current loss 0.013313872407888994\n",
      "epoch 52, batch 30, current loss 0.009265131474239751\n",
      "epoch 53, batch 10, current loss 0.03229152244166471\n",
      "epoch 53, batch 20, current loss 0.014150891295867041\n",
      "epoch 53, batch 30, current loss 0.005812657812202815\n",
      "epoch 54, batch 10, current loss 0.008225060874246992\n",
      "epoch 54, batch 20, current loss 0.02906643032329157\n",
      "epoch 54, batch 30, current loss 0.006143836065893993\n",
      "epoch 55, batch 10, current loss 0.010463794492534362\n",
      "epoch 55, batch 20, current loss 0.015847127634333446\n",
      "epoch 55, batch 30, current loss 0.0038619999890215696\n",
      "epoch 56, batch 10, current loss 0.015123631723690778\n",
      "epoch 56, batch 20, current loss 0.002885207286453806\n",
      "epoch 56, batch 30, current loss 0.07237117247641436\n",
      "epoch 57, batch 10, current loss 0.02237110477872193\n",
      "epoch 57, batch 20, current loss 0.012106185732409357\n",
      "epoch 57, batch 30, current loss 0.009838450106326491\n",
      "epoch 58, batch 10, current loss 0.0034783625189447774\n",
      "epoch 58, batch 20, current loss 0.0066240305401152\n",
      "epoch 58, batch 30, current loss 0.005955164373153821\n",
      "epoch 59, batch 10, current loss 0.001692569680744782\n",
      "epoch 59, batch 20, current loss 0.0029890985402744263\n",
      "epoch 59, batch 30, current loss 0.012673540084506385\n",
      "epoch 60, batch 10, current loss 0.01130511507508345\n",
      "epoch 60, batch 20, current loss 0.004353977942082565\n",
      "epoch 60, batch 30, current loss 0.002881483267992735\n",
      "epoch 60 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 60 test accuracy: \n",
      "total_num:96, test accuracy:0.96875, positive_acc:0.9871794871794872, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  1.  77.]]\n",
      "[0, 90]\n",
      "[38]\n",
      "epoch 61, batch 10, current loss 0.003214942986960523\n",
      "epoch 61, batch 20, current loss 0.004108351783361286\n",
      "epoch 61, batch 30, current loss 0.004959454259369523\n",
      "epoch 62, batch 10, current loss 0.0113851690737647\n",
      "epoch 62, batch 20, current loss 0.007145734427467687\n",
      "epoch 62, batch 30, current loss 0.003779769114044029\n",
      "epoch 63, batch 10, current loss 0.001861771455151029\n",
      "epoch 63, batch 20, current loss 0.002443753070838284\n",
      "epoch 63, batch 30, current loss 0.007744033174822107\n",
      "epoch 64, batch 10, current loss 0.0030268868256825955\n",
      "epoch 64, batch 20, current loss 0.0026051238921354523\n",
      "epoch 64, batch 30, current loss 0.010842937333654845\n",
      "epoch 65, batch 10, current loss 0.10112887223949656\n",
      "epoch 65, batch 20, current loss 0.08329870931338519\n",
      "epoch 65, batch 30, current loss 0.05487651191651821\n",
      "epoch 66, batch 10, current loss 0.050144051923416554\n",
      "epoch 66, batch 20, current loss 0.04583620930206962\n",
      "epoch 66, batch 30, current loss 0.006822704197838902\n",
      "epoch 67, batch 10, current loss 0.020283512637251987\n",
      "epoch 67, batch 20, current loss 0.023777222813805565\n",
      "epoch 67, batch 30, current loss 0.005297511856770143\n",
      "epoch 68, batch 10, current loss 0.01423721450555604\n",
      "epoch 68, batch 20, current loss 0.005790353829797823\n",
      "epoch 68, batch 30, current loss 0.01721804365515709\n",
      "epoch 69, batch 10, current loss 0.009799954963091296\n",
      "epoch 69, batch 20, current loss 0.0026455102022737266\n",
      "epoch 69, batch 30, current loss 0.021806761366315187\n",
      "epoch 70, batch 10, current loss 0.01929618391877739\n",
      "epoch 70, batch 20, current loss 0.00857423422858119\n",
      "epoch 70, batch 30, current loss 0.011304876160284038\n",
      "epoch 70 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 70 test accuracy: \n",
      "total_num:96, test accuracy:0.9375, positive_acc:1.0, negative_acc:0.6666666666666666\n",
      "[[ 12.   6.]\n",
      " [  0.  78.]]\n",
      "[0, 39, 78, 86, 88, 90]\n",
      "[]\n",
      "epoch 71, batch 10, current loss 0.008093465003184974\n",
      "epoch 71, batch 20, current loss 0.003931896717404015\n",
      "epoch 71, batch 30, current loss 0.008383321072324179\n",
      "epoch 72, batch 10, current loss 0.0020019234289065935\n",
      "epoch 72, batch 20, current loss 0.0016812696238048374\n",
      "epoch 72, batch 30, current loss 0.004002945240790723\n",
      "epoch 73, batch 10, current loss 0.0026103232157765888\n",
      "epoch 73, batch 20, current loss 0.02604195097956108\n",
      "epoch 73, batch 30, current loss 0.007090142820379697\n",
      "epoch 74, batch 10, current loss 0.012870300601935013\n",
      "epoch 74, batch 20, current loss 0.004806270608241903\n",
      "epoch 74, batch 30, current loss 0.014082735679403413\n",
      "epoch 75, batch 10, current loss 0.0011467755008197854\n",
      "epoch 75, batch 20, current loss 0.007591211026010569\n",
      "epoch 75, batch 30, current loss 0.00252892222488299\n",
      "epoch 76, batch 10, current loss 0.0015516226332692896\n",
      "epoch 76, batch 20, current loss 0.0022093424297054296\n",
      "epoch 76, batch 30, current loss 0.007389246649108827\n",
      "epoch 77, batch 10, current loss 0.0012455162577680312\n",
      "epoch 77, batch 20, current loss 0.003233097039628774\n",
      "epoch 77, batch 30, current loss 0.008517248735006432\n",
      "epoch 78, batch 10, current loss 0.016234352206811308\n",
      "epoch 78, batch 20, current loss 0.0235609194758581\n",
      "epoch 78, batch 30, current loss 0.02141723364038626\n",
      "epoch 79, batch 10, current loss 0.022759839266655035\n",
      "epoch 79, batch 20, current loss 0.025733576335187535\n",
      "epoch 79, batch 30, current loss 0.0038119811040814968\n",
      "epoch 80, batch 10, current loss 0.05404319860099349\n",
      "epoch 80, batch 20, current loss 0.0018453686527209357\n",
      "epoch 80, batch 30, current loss 0.020959805941674858\n",
      "epoch 80 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 80 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9615384615384616, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  3.  75.]]\n",
      "[0, 90]\n",
      "[32, 38, 46]\n",
      "epoch 81, batch 10, current loss 0.030247750282433115\n",
      "epoch 81, batch 20, current loss 0.02366418124001939\n",
      "epoch 81, batch 30, current loss 0.00800246774306288\n",
      "epoch 82, batch 10, current loss 0.02035601514362497\n",
      "epoch 82, batch 20, current loss 0.014472262341587338\n",
      "epoch 82, batch 30, current loss 0.006375522195594385\n",
      "epoch 83, batch 10, current loss 0.002810097638575826\n",
      "epoch 83, batch 20, current loss 0.0015025680040707812\n",
      "epoch 83, batch 30, current loss 0.016681031842017545\n",
      "epoch 84, batch 10, current loss 0.007857446082925889\n",
      "epoch 84, batch 20, current loss 0.038157002010120775\n",
      "epoch 84, batch 30, current loss 0.0028322447797108907\n",
      "epoch 85, batch 10, current loss 0.002563838116111583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85, batch 20, current loss 0.002388770259858575\n",
      "epoch 85, batch 30, current loss 0.008559757335751783\n",
      "epoch 86, batch 10, current loss 0.009992879352648743\n",
      "epoch 86, batch 20, current loss 0.009805998866795562\n",
      "epoch 86, batch 30, current loss 0.0020526052270724905\n",
      "epoch 87, batch 10, current loss 0.01956308233930031\n",
      "epoch 87, batch 20, current loss 0.023049670239561237\n",
      "epoch 87, batch 30, current loss 0.0030625476269051433\n",
      "epoch 88, batch 10, current loss 0.007672017259028508\n",
      "epoch 88, batch 20, current loss 0.006354524944617879\n",
      "epoch 88, batch 30, current loss 0.0022551067297172265\n",
      "epoch 89, batch 10, current loss 0.007615884681581519\n",
      "epoch 89, batch 20, current loss 0.04131485071411589\n",
      "epoch 89, batch 30, current loss 0.009122251775988844\n",
      "epoch 90, batch 10, current loss 0.0031225039609125817\n",
      "epoch 90, batch 20, current loss 0.0021590915326669346\n",
      "epoch 90, batch 30, current loss 0.0007471209828509018\n",
      "epoch 90 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 90 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9615384615384616, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  3.  75.]]\n",
      "[0, 90]\n",
      "[32, 38, 46]\n",
      "epoch 91, batch 10, current loss 0.0024335174675798044\n",
      "epoch 91, batch 20, current loss 0.035747040190108235\n",
      "epoch 91, batch 30, current loss 0.002838161084218882\n",
      "epoch 92, batch 10, current loss 0.005972661967098248\n",
      "epoch 92, batch 20, current loss 0.007274957641493529\n",
      "epoch 92, batch 30, current loss 0.013736523396801203\n",
      "epoch 93, batch 10, current loss 0.00694730473915115\n",
      "epoch 93, batch 20, current loss 0.004883065680041909\n",
      "epoch 93, batch 30, current loss 0.11263405835488811\n",
      "epoch 94, batch 10, current loss 0.020218787516932936\n",
      "epoch 94, batch 20, current loss 0.16634862727951258\n",
      "epoch 94, batch 30, current loss 0.036023388186004014\n",
      "epoch 95, batch 10, current loss 0.017601459147408606\n",
      "epoch 95, batch 20, current loss 0.04263914487673901\n",
      "epoch 95, batch 30, current loss 0.009039296268019826\n",
      "epoch 96, batch 10, current loss 0.01066225123067852\n",
      "epoch 96, batch 20, current loss 0.019780894543509932\n",
      "epoch 96, batch 30, current loss 0.005754698324017227\n",
      "epoch 97, batch 10, current loss 0.041746957908617335\n",
      "epoch 97, batch 20, current loss 0.0044891433288285045\n",
      "epoch 97, batch 30, current loss 0.010460539706400596\n",
      "epoch 98, batch 10, current loss 0.0028705400793114675\n",
      "epoch 98, batch 20, current loss 0.004149547073757276\n",
      "epoch 98, batch 30, current loss 0.004638526509370422\n",
      "epoch 99, batch 10, current loss 0.0048226053404505365\n",
      "epoch 99, batch 20, current loss 0.006302588652397389\n",
      "epoch 99, batch 30, current loss 0.00247689621464815\n",
      "epoch 100, batch 10, current loss 0.004634536651428789\n",
      "epoch 100, batch 20, current loss 0.0025833680818323048\n",
      "epoch 100, batch 30, current loss 0.03134920613483701\n",
      "epoch 100 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 100 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 101, batch 10, current loss 0.0017766677410691045\n",
      "epoch 101, batch 20, current loss 0.0033379220552888\n",
      "epoch 101, batch 30, current loss 0.00037644202675437557\n",
      "epoch 102, batch 10, current loss 0.002577049221872585\n",
      "epoch 102, batch 20, current loss 0.0013227456118329428\n",
      "epoch 102, batch 30, current loss 0.005180389034649124\n",
      "epoch 103, batch 10, current loss 0.002583330680954532\n",
      "epoch 103, batch 20, current loss 0.002858657282195054\n",
      "epoch 103, batch 30, current loss 0.01221044463236467\n",
      "epoch 104, batch 10, current loss 0.0037780956161441283\n",
      "epoch 104, batch 20, current loss 0.0014070982360863127\n",
      "epoch 104, batch 30, current loss 0.0066091013395634945\n",
      "epoch 105, batch 10, current loss 0.0007654711938812397\n",
      "epoch 105, batch 20, current loss 0.0024026461302128154\n",
      "epoch 105, batch 30, current loss 0.0021173482440644876\n",
      "epoch 106, batch 10, current loss 0.00039935543354658877\n",
      "epoch 106, batch 20, current loss 0.0019938119105063377\n",
      "epoch 106, batch 30, current loss 0.0029526973521569744\n",
      "epoch 107, batch 10, current loss 0.0016321826402418082\n",
      "epoch 107, batch 20, current loss 0.0047749365750860305\n",
      "epoch 107, batch 30, current loss 0.0021491193314432167\n",
      "epoch 108, batch 10, current loss 0.0034250403608893976\n",
      "epoch 108, batch 20, current loss 0.00547393318011018\n",
      "epoch 108, batch 30, current loss 0.0014530871412716806\n",
      "epoch 109, batch 10, current loss 0.004512239411997143\n",
      "epoch 109, batch 20, current loss 0.000868320601148298\n",
      "epoch 109, batch 30, current loss 0.001753660780377686\n",
      "epoch 110, batch 10, current loss 0.002474381375213852\n",
      "epoch 110, batch 20, current loss 0.000860234545689309\n",
      "epoch 110, batch 30, current loss 0.0009252284857211634\n",
      "epoch 110 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 110 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9615384615384616, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  3.  75.]]\n",
      "[0, 90]\n",
      "[32, 38, 46]\n",
      "epoch 111, batch 10, current loss 0.004127576387691079\n",
      "epoch 111, batch 20, current loss 0.0017335282274871134\n",
      "epoch 111, batch 30, current loss 0.001394326111767441\n",
      "epoch 112, batch 10, current loss 0.0008646604364912491\n",
      "epoch 112, batch 20, current loss 0.0007992166370968335\n",
      "epoch 112, batch 30, current loss 0.021266863821074365\n",
      "epoch 113, batch 10, current loss 0.0015243617170199286\n",
      "epoch 113, batch 20, current loss 0.0031036134052556006\n",
      "epoch 113, batch 30, current loss 0.0005696140964573715\n",
      "epoch 114, batch 10, current loss 0.0010220708441920578\n",
      "epoch 114, batch 20, current loss 0.0008410104775975924\n",
      "epoch 114, batch 30, current loss 0.0022419145781896075\n",
      "epoch 115, batch 10, current loss 0.0024145079725713\n",
      "epoch 115, batch 20, current loss 0.0034644458944967484\n",
      "epoch 115, batch 30, current loss 0.0008025614348298405\n",
      "epoch 116, batch 10, current loss 0.00031219707962009123\n",
      "epoch 116, batch 20, current loss 0.0018171507770603057\n",
      "epoch 116, batch 30, current loss 0.0010343146917875857\n",
      "epoch 117, batch 10, current loss 0.0021391584541561315\n",
      "epoch 117, batch 20, current loss 0.0011675477522658183\n",
      "epoch 117, batch 30, current loss 0.0006182766352139879\n",
      "epoch 118, batch 10, current loss 0.0014669731186586431\n",
      "epoch 118, batch 20, current loss 0.0004105732943571638\n",
      "epoch 118, batch 30, current loss 0.0005012195942981634\n",
      "epoch 119, batch 10, current loss 0.0010769646967673907\n",
      "epoch 119, batch 20, current loss 0.000422196319050272\n",
      "epoch 119, batch 30, current loss 0.0002617177324282238\n",
      "epoch 120, batch 10, current loss 0.0007148379008867778\n",
      "epoch 120, batch 20, current loss 0.004578136666168575\n",
      "epoch 120, batch 30, current loss 0.0019446780694124754\n",
      "epoch 120 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 120 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 121, batch 10, current loss 0.002621197955159005\n",
      "epoch 121, batch 20, current loss 0.0015626716558472253\n",
      "epoch 121, batch 30, current loss 0.0024929322771640727\n",
      "epoch 122, batch 10, current loss 0.006395144708949374\n",
      "epoch 122, batch 20, current loss 0.012912549175234744\n",
      "epoch 122, batch 30, current loss 0.0009343795340100769\n",
      "epoch 123, batch 10, current loss 0.000886395710404031\n",
      "epoch 123, batch 20, current loss 0.00040718318614381134\n",
      "epoch 123, batch 30, current loss 0.008883786378282821\n",
      "epoch 124, batch 10, current loss 0.0005739655302022584\n",
      "epoch 124, batch 20, current loss 0.0009983400232158601\n",
      "epoch 124, batch 30, current loss 0.0017288071772782131\n",
      "epoch 125, batch 10, current loss 0.0014630586352723184\n",
      "epoch 125, batch 20, current loss 0.0030681665049996807\n",
      "epoch 125, batch 30, current loss 0.0005843132261361461\n",
      "epoch 126, batch 10, current loss 0.0008936375212215353\n",
      "epoch 126, batch 20, current loss 0.00047533958659187194\n",
      "epoch 126, batch 30, current loss 0.0003343674721691059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 127, batch 10, current loss 0.003338714539495413\n",
      "epoch 127, batch 20, current loss 0.0007178973013651557\n",
      "epoch 127, batch 30, current loss 0.008238878671545536\n",
      "epoch 128, batch 10, current loss 0.0010565154734649695\n",
      "epoch 128, batch 20, current loss 0.0013237244813353755\n",
      "epoch 128, batch 30, current loss 0.0005623671710054623\n",
      "epoch 129, batch 10, current loss 0.0017398396150383633\n",
      "epoch 129, batch 20, current loss 0.0005143883850905695\n",
      "epoch 129, batch 30, current loss 0.0010936846818367485\n",
      "epoch 130, batch 10, current loss 0.0016932637197896838\n",
      "epoch 130, batch 20, current loss 0.001250858411731315\n",
      "epoch 130, batch 30, current loss 0.0008873977392795495\n",
      "epoch 130 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 130 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9743589743589743, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  2.  76.]]\n",
      "[0, 88, 90]\n",
      "[32, 38]\n",
      "epoch 131, batch 10, current loss 0.0008741419333091471\n",
      "epoch 131, batch 20, current loss 0.0009904788366839057\n",
      "epoch 131, batch 30, current loss 0.016326864596703673\n",
      "epoch 132, batch 10, current loss 0.001367383014439838\n",
      "epoch 132, batch 20, current loss 0.0004016897590190638\n",
      "epoch 132, batch 30, current loss 0.0010620655262755462\n",
      "epoch 133, batch 10, current loss 0.0014270584251789842\n",
      "epoch 133, batch 20, current loss 0.00031304351214203054\n",
      "epoch 133, batch 30, current loss 0.003577808116460801\n",
      "epoch 134, batch 10, current loss 0.0008230326953707845\n",
      "epoch 134, batch 20, current loss 0.0012921258246933575\n",
      "epoch 134, batch 30, current loss 0.0019714288533577927\n",
      "epoch 135, batch 10, current loss 0.0036229060251571354\n",
      "epoch 135, batch 20, current loss 0.00018745772458714783\n",
      "epoch 135, batch 30, current loss 0.00038552651640202387\n",
      "epoch 136, batch 10, current loss 0.0003746535876416601\n",
      "epoch 136, batch 20, current loss 0.00014459225985774538\n",
      "epoch 136, batch 30, current loss 0.0005041702490416356\n",
      "epoch 137, batch 10, current loss 0.001585632999012887\n",
      "epoch 137, batch 20, current loss 0.0042819527225219645\n",
      "epoch 137, batch 30, current loss 0.0029704842880164508\n",
      "epoch 138, batch 10, current loss 0.0007249078353197547\n",
      "epoch 138, batch 20, current loss 0.0006558567041793139\n",
      "epoch 138, batch 30, current loss 0.0014661621775303503\n",
      "epoch 139, batch 10, current loss 0.0005900889416807331\n",
      "epoch 139, batch 20, current loss 0.0010280322741891724\n",
      "epoch 139, batch 30, current loss 0.00024384380485571456\n",
      "epoch 140, batch 10, current loss 0.0004353503609308973\n",
      "epoch 140, batch 20, current loss 0.002252363972365856\n",
      "epoch 140, batch 30, current loss 0.00024633066950627833\n",
      "epoch 140 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 140 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 141, batch 10, current loss 0.0013508982061466667\n",
      "epoch 141, batch 20, current loss 0.000524447506541037\n",
      "epoch 141, batch 30, current loss 0.0011296396321995416\n",
      "epoch 142, batch 10, current loss 0.0005541640111914603\n",
      "epoch 142, batch 20, current loss 0.0005037766823079437\n",
      "epoch 142, batch 30, current loss 0.0015305990034903516\n",
      "epoch 143, batch 10, current loss 0.001823655052430695\n",
      "epoch 143, batch 20, current loss 0.001231730520248675\n",
      "epoch 143, batch 30, current loss 0.00048012246152211445\n",
      "epoch 144, batch 10, current loss 0.0005064274184405804\n",
      "epoch 144, batch 20, current loss 0.0008239531696744961\n",
      "epoch 144, batch 30, current loss 0.0007191556269390276\n",
      "epoch 145, batch 10, current loss 0.0004928728287268314\n",
      "epoch 145, batch 20, current loss 0.0005647663700983685\n",
      "epoch 145, batch 30, current loss 0.002396851209960005\n",
      "epoch 146, batch 10, current loss 0.0008492615175782703\n",
      "epoch 146, batch 20, current loss 0.00021483946202351944\n",
      "epoch 146, batch 30, current loss 0.00033027496792783497\n",
      "epoch 147, batch 10, current loss 0.0011983402751866378\n",
      "epoch 147, batch 20, current loss 0.00021967981974739813\n",
      "epoch 147, batch 30, current loss 0.008555477939808043\n",
      "epoch 148, batch 10, current loss 0.0005390978165451088\n",
      "epoch 148, batch 20, current loss 0.00036563686371664516\n",
      "epoch 148, batch 30, current loss 0.000420487990413676\n",
      "epoch 149, batch 10, current loss 0.00039646057630307043\n",
      "epoch 149, batch 20, current loss 0.0002456501586493687\n",
      "epoch 149, batch 30, current loss 0.0007394655722691823\n",
      "epoch 150, batch 10, current loss 0.0005204207145652617\n",
      "epoch 150, batch 20, current loss 0.0017175903063616715\n",
      "epoch 150, batch 30, current loss 0.0002392648938439379\n",
      "epoch 150 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 150 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9615384615384616, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  3.  75.]]\n",
      "[0, 90]\n",
      "[32, 38, 45]\n",
      "epoch 151, batch 10, current loss 0.00030152070557960543\n",
      "epoch 151, batch 20, current loss 0.0013708415189285007\n",
      "epoch 151, batch 30, current loss 0.00022288356904027752\n",
      "epoch 152, batch 10, current loss 0.0003783617599765421\n",
      "epoch 152, batch 20, current loss 0.0006150125722342636\n",
      "epoch 152, batch 30, current loss 0.00010780512175188051\n",
      "epoch 153, batch 10, current loss 0.002700078087946167\n",
      "epoch 153, batch 20, current loss 0.00018760369493975304\n",
      "epoch 153, batch 30, current loss 0.0007280381529199076\n",
      "epoch 154, batch 10, current loss 0.0002464158644215786\n",
      "epoch 154, batch 20, current loss 0.00048557835671090286\n",
      "epoch 154, batch 30, current loss 0.000342632433239487\n",
      "epoch 155, batch 10, current loss 0.00041397729146410713\n",
      "epoch 155, batch 20, current loss 0.00032603447252768094\n",
      "epoch 155, batch 30, current loss 0.0011834664877824253\n",
      "epoch 156, batch 10, current loss 0.00030318872377392835\n",
      "epoch 156, batch 20, current loss 0.0004099412166397087\n",
      "epoch 156, batch 30, current loss 0.0005146668761881301\n",
      "epoch 157, batch 10, current loss 0.000525836842280114\n",
      "epoch 157, batch 20, current loss 0.004881835202195361\n",
      "epoch 157, batch 30, current loss 0.00033698218594508945\n",
      "epoch 158, batch 10, current loss 0.0009877200616756454\n",
      "epoch 158, batch 20, current loss 0.00017475169170211302\n",
      "epoch 158, batch 30, current loss 0.0003043533204618143\n",
      "epoch 159, batch 10, current loss 0.0008778205046837684\n",
      "epoch 159, batch 20, current loss 0.00013328735294635408\n",
      "epoch 159, batch 30, current loss 0.00041586459456084414\n",
      "epoch 160, batch 10, current loss 0.0002885757019612356\n",
      "epoch 160, batch 20, current loss 0.0029814580051606755\n",
      "epoch 160, batch 30, current loss 0.001163982693287835\n",
      "epoch 160 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 160 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9615384615384616, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  3.  75.]]\n",
      "[0, 90]\n",
      "[32, 38, 45]\n",
      "epoch 161, batch 10, current loss 0.0020066156441316705\n",
      "epoch 161, batch 20, current loss 0.00032767630873422606\n",
      "epoch 161, batch 30, current loss 0.0004296758332202444\n",
      "epoch 162, batch 10, current loss 0.0031868684283836044\n",
      "epoch 162, batch 20, current loss 0.00023209861392388121\n",
      "epoch 162, batch 30, current loss 0.00261191227400559\n",
      "epoch 163, batch 10, current loss 0.00010326927695132326\n",
      "epoch 163, batch 20, current loss 0.004216125142920646\n",
      "epoch 163, batch 30, current loss 0.0014268403025198494\n",
      "epoch 164, batch 10, current loss 0.000572713924702839\n",
      "epoch 164, batch 20, current loss 0.001344589709333377\n",
      "epoch 164, batch 30, current loss 0.0009188709853333421\n",
      "epoch 165, batch 10, current loss 0.0005122411705087871\n",
      "epoch 165, batch 20, current loss 0.0004210090988635784\n",
      "epoch 165, batch 30, current loss 0.0007029876520391554\n",
      "epoch 166, batch 10, current loss 0.00046496891845890784\n",
      "epoch 166, batch 20, current loss 0.00024682937410034354\n",
      "epoch 166, batch 30, current loss 0.00010774838137876941\n",
      "epoch 167, batch 10, current loss 0.00040347378453589047\n",
      "epoch 167, batch 20, current loss 0.0014949827795135207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 167, batch 30, current loss 0.0006327678349407507\n",
      "epoch 168, batch 10, current loss 0.00013065386556263548\n",
      "epoch 168, batch 20, current loss 0.0023369872567855056\n",
      "epoch 168, batch 30, current loss 0.0004822729597435682\n",
      "epoch 169, batch 10, current loss 0.0005331634672984364\n",
      "epoch 169, batch 20, current loss 0.00027905124134122163\n",
      "epoch 169, batch 30, current loss 9.895631737890653e-05\n",
      "epoch 170, batch 10, current loss 0.0008335155676832074\n",
      "epoch 170, batch 20, current loss 0.0020967313379514963\n",
      "epoch 170, batch 30, current loss 0.00033171818613482175\n",
      "epoch 170 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 170 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 171, batch 10, current loss 0.00032361195735575166\n",
      "epoch 171, batch 20, current loss 0.00164918672398926\n",
      "epoch 171, batch 30, current loss 0.0008195375867217081\n",
      "epoch 172, batch 10, current loss 0.0002621972211272805\n",
      "epoch 172, batch 20, current loss 0.0007791948177327867\n",
      "epoch 172, batch 30, current loss 0.00035217724253016057\n",
      "epoch 173, batch 10, current loss 0.00014386515013029567\n",
      "epoch 173, batch 20, current loss 0.0023032355900795666\n",
      "epoch 173, batch 30, current loss 0.0006500927178421989\n",
      "epoch 174, batch 10, current loss 0.0011454177634732331\n",
      "epoch 174, batch 20, current loss 0.0014703936256410088\n",
      "epoch 174, batch 30, current loss 0.0006495706671557855\n",
      "epoch 175, batch 10, current loss 0.0011525683786203444\n",
      "epoch 175, batch 20, current loss 0.00012201530116726645\n",
      "epoch 175, batch 30, current loss 0.006567067528521875\n",
      "epoch 176, batch 10, current loss 0.0011468423408587114\n",
      "epoch 176, batch 20, current loss 0.00038103297301859127\n",
      "epoch 176, batch 30, current loss 0.0008653622819110752\n",
      "epoch 177, batch 10, current loss 0.0016287433021716425\n",
      "epoch 177, batch 20, current loss 0.00042335897087468765\n",
      "epoch 177, batch 30, current loss 0.000433455262282223\n",
      "epoch 178, batch 10, current loss 0.00048061008856166155\n",
      "epoch 178, batch 20, current loss 0.0022418716613174182\n",
      "epoch 178, batch 30, current loss 0.000840921281996998\n",
      "epoch 179, batch 10, current loss 0.00041913528912118636\n",
      "epoch 179, batch 20, current loss 0.0003843766713544028\n",
      "epoch 179, batch 30, current loss 0.0009320175087850658\n",
      "epoch 180, batch 10, current loss 0.0009126544940954773\n",
      "epoch 180, batch 20, current loss 0.001776962754684064\n",
      "epoch 180, batch 30, current loss 0.00014920404710210277\n",
      "epoch 180 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 180 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9743589743589743, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  2.  76.]]\n",
      "[0, 88, 90]\n",
      "[32, 38]\n",
      "epoch 181, batch 10, current loss 0.0002562078339906293\n",
      "epoch 181, batch 20, current loss 0.00014909861679370806\n",
      "epoch 181, batch 30, current loss 0.00039776008707121947\n",
      "epoch 182, batch 10, current loss 0.0009797777349376702\n",
      "epoch 182, batch 20, current loss 0.000481636792756035\n",
      "epoch 182, batch 30, current loss 0.00025365491446791564\n",
      "epoch 183, batch 10, current loss 0.0023525774962763536\n",
      "epoch 183, batch 20, current loss 0.00016165372917384958\n",
      "epoch 183, batch 30, current loss 0.000998746481309354\n",
      "epoch 184, batch 10, current loss 0.0002291002165293321\n",
      "epoch 184, batch 20, current loss 0.00035489700094331055\n",
      "epoch 184, batch 30, current loss 0.00302794644721871\n",
      "epoch 185, batch 10, current loss 0.001438277224679041\n",
      "epoch 185, batch 20, current loss 0.000652084488683613\n",
      "epoch 185, batch 30, current loss 0.0002527800125790236\n",
      "epoch 186, batch 10, current loss 0.0006339762221614365\n",
      "epoch 186, batch 20, current loss 0.0002596113208710449\n",
      "epoch 186, batch 30, current loss 0.0003908905426214915\n",
      "epoch 187, batch 10, current loss 0.000389686625203467\n",
      "epoch 187, batch 20, current loss 0.000583499802996812\n",
      "epoch 187, batch 30, current loss 0.00013135839617461897\n",
      "epoch 188, batch 10, current loss 0.0003886768381562433\n",
      "epoch 188, batch 20, current loss 0.0006897768114868085\n",
      "epoch 188, batch 30, current loss 0.01509360831187223\n",
      "epoch 189, batch 10, current loss 0.0002886267719077296\n",
      "epoch 189, batch 20, current loss 0.0003139624815048592\n",
      "epoch 189, batch 30, current loss 0.000482772466057213\n",
      "epoch 190, batch 10, current loss 0.0002183235666961991\n",
      "epoch 190, batch 20, current loss 0.0003131191004285938\n",
      "epoch 190, batch 30, current loss 0.0014845262079688838\n",
      "epoch 190 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 190 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9743589743589743, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  2.  76.]]\n",
      "[0, 88, 90]\n",
      "[32, 38]\n",
      "epoch 191, batch 10, current loss 0.0005967420258457424\n",
      "epoch 191, batch 20, current loss 0.0009564945881720633\n",
      "epoch 191, batch 30, current loss 0.00032531884612581053\n",
      "epoch 192, batch 10, current loss 0.000498790138590266\n",
      "epoch 192, batch 20, current loss 0.00016150370065588505\n",
      "epoch 192, batch 30, current loss 0.00030272316034825054\n",
      "epoch 193, batch 10, current loss 0.001508969060432719\n",
      "epoch 193, batch 20, current loss 0.00019953994251409313\n",
      "epoch 193, batch 30, current loss 0.0005300975079080672\n",
      "epoch 194, batch 10, current loss 0.0010549412234468036\n",
      "epoch 194, batch 20, current loss 0.0005288855349135701\n",
      "epoch 194, batch 30, current loss 0.0012292036626604386\n",
      "epoch 195, batch 10, current loss 0.0005109052362968214\n",
      "epoch 195, batch 20, current loss 0.0002274833116644004\n",
      "epoch 195, batch 30, current loss 0.00012812736440537265\n",
      "epoch 196, batch 10, current loss 0.0006157336152682547\n",
      "epoch 196, batch 20, current loss 0.00020847856194450287\n",
      "epoch 196, batch 30, current loss 0.00023672908355365507\n",
      "epoch 197, batch 10, current loss 0.0001184871600344195\n",
      "epoch 197, batch 20, current loss 0.00014428078266064402\n",
      "epoch 197, batch 30, current loss 0.0008687804975124891\n",
      "epoch 198, batch 10, current loss 0.00014295423052317345\n",
      "epoch 198, batch 20, current loss 0.00029512829351006074\n",
      "epoch 198, batch 30, current loss 0.0009801980901102069\n",
      "epoch 199, batch 10, current loss 0.0018021001322267693\n",
      "epoch 199, batch 20, current loss 0.0004718400499768904\n",
      "epoch 199, batch 30, current loss 0.0002529897312342655\n",
      "epoch 200, batch 10, current loss 0.0015240906766848638\n",
      "epoch 200, batch 20, current loss 0.00027726990538212704\n",
      "epoch 200, batch 30, current loss 0.0004242022760081454\n",
      "epoch 200 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 200 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9743589743589743, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  2.  76.]]\n",
      "[0, 88, 90]\n",
      "[32, 38]\n",
      "epoch 201, batch 10, current loss 0.0005419600751793042\n",
      "epoch 201, batch 20, current loss 0.0001803403111807711\n",
      "epoch 201, batch 30, current loss 0.00033155015098600414\n",
      "epoch 202, batch 10, current loss 0.00015691597018303584\n",
      "epoch 202, batch 20, current loss 0.0006482262405825168\n",
      "epoch 202, batch 30, current loss 0.0005070393604000856\n",
      "epoch 203, batch 10, current loss 0.0003904742348822765\n",
      "epoch 203, batch 20, current loss 0.0020234001569406244\n",
      "epoch 203, batch 30, current loss 0.0007647772985365009\n",
      "epoch 204, batch 10, current loss 0.0016666452560457402\n",
      "epoch 204, batch 20, current loss 0.00011994259161838273\n",
      "epoch 204, batch 30, current loss 0.00012490514827732114\n",
      "epoch 205, batch 10, current loss 0.0002507174451238825\n",
      "epoch 205, batch 20, current loss 0.00015226893497128914\n",
      "epoch 205, batch 30, current loss 0.00036002230790472823\n",
      "epoch 206, batch 10, current loss 0.0004714192264145822\n",
      "epoch 206, batch 20, current loss 0.0005697049071386572\n",
      "epoch 206, batch 30, current loss 6.0124268202343956e-05\n",
      "epoch 207, batch 10, current loss 0.0005556611569772941\n",
      "epoch 207, batch 20, current loss 7.235925568238599e-05\n",
      "epoch 207, batch 30, current loss 0.00022918207132534009\n",
      "epoch 208, batch 10, current loss 0.00018905191100202501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 208, batch 20, current loss 0.0009519442610326223\n",
      "epoch 208, batch 30, current loss 0.00036354849789859144\n",
      "epoch 209, batch 10, current loss 0.00021243767187115737\n",
      "epoch 209, batch 20, current loss 0.001425344687413599\n",
      "epoch 209, batch 30, current loss 0.0004905563002466807\n",
      "epoch 210, batch 10, current loss 0.0007825203029824479\n",
      "epoch 210, batch 20, current loss 0.00021382989498306416\n",
      "epoch 210, batch 30, current loss 0.001093894863879541\n",
      "epoch 210 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 210 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9743589743589743, negative_acc:0.8333333333333334\n",
      "[[ 15.   3.]\n",
      " [  2.  76.]]\n",
      "[0, 88, 90]\n",
      "[32, 38]\n",
      "epoch 211, batch 10, current loss 0.002622373535996303\n",
      "epoch 211, batch 20, current loss 0.0003258159807046468\n",
      "epoch 211, batch 30, current loss 0.00045782879960825084\n",
      "epoch 212, batch 10, current loss 0.00028708360259770415\n",
      "epoch 212, batch 20, current loss 0.0005359833121474367\n",
      "epoch 212, batch 30, current loss 0.0003195938660155662\n",
      "epoch 213, batch 10, current loss 0.0005614705482912541\n",
      "epoch 213, batch 20, current loss 0.00018916574572358513\n",
      "epoch 213, batch 30, current loss 0.0004276800980733242\n",
      "epoch 214, batch 10, current loss 0.0002929160706116818\n",
      "epoch 214, batch 20, current loss 0.0014633605830567831\n",
      "epoch 214, batch 30, current loss 0.0005032667419072823\n",
      "epoch 215, batch 10, current loss 0.0005147586551174754\n",
      "epoch 215, batch 20, current loss 0.0002449809480822296\n",
      "epoch 215, batch 30, current loss 0.0007174904068961041\n",
      "epoch 216, batch 10, current loss 0.00029162549053580734\n",
      "epoch 216, batch 20, current loss 0.0007261246668349486\n",
      "epoch 216, batch 30, current loss 0.0003921241961506894\n",
      "epoch 217, batch 10, current loss 0.0002655043313097849\n",
      "epoch 217, batch 20, current loss 0.0003610072564697475\n",
      "epoch 217, batch 30, current loss 0.0043232387896750876\n",
      "epoch 218, batch 10, current loss 0.0003673986720059474\n",
      "epoch 218, batch 20, current loss 0.0004323873150497093\n",
      "epoch 218, batch 30, current loss 0.00017472240938332106\n",
      "epoch 219, batch 10, current loss 0.0007790368220412347\n",
      "epoch 219, batch 20, current loss 0.00030292876426756264\n",
      "epoch 219, batch 30, current loss 0.00018053928288281896\n",
      "epoch 220, batch 10, current loss 0.0007066251757350984\n",
      "epoch 220, batch 20, current loss 0.00019099433566225344\n",
      "epoch 220, batch 30, current loss 0.00017089602906708023\n",
      "epoch 220 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 220 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 221, batch 10, current loss 6.098925914557185e-05\n",
      "epoch 221, batch 20, current loss 0.0006653991262282943\n",
      "epoch 221, batch 30, current loss 0.00019802253118541557\n",
      "epoch 222, batch 10, current loss 8.536034656572155e-05\n",
      "epoch 222, batch 20, current loss 0.00034894850587079417\n",
      "epoch 222, batch 30, current loss 0.00010641357321219402\n",
      "epoch 223, batch 10, current loss 0.00018438541974319378\n",
      "epoch 223, batch 20, current loss 0.0008136190282129974\n",
      "epoch 223, batch 30, current loss 0.00028666474727288003\n",
      "epoch 224, batch 10, current loss 0.00018527322354202624\n",
      "epoch 224, batch 20, current loss 0.00012248352359165438\n",
      "epoch 224, batch 30, current loss 0.00017583005214873992\n",
      "epoch 225, batch 10, current loss 0.000581188825344725\n",
      "epoch 225, batch 20, current loss 0.0002050629238510737\n",
      "epoch 225, batch 30, current loss 0.0008867672267115267\n",
      "epoch 226, batch 10, current loss 0.0020611744953384914\n",
      "epoch 226, batch 20, current loss 0.0008343191179847054\n",
      "epoch 226, batch 30, current loss 0.001935568144563149\n",
      "epoch 227, batch 10, current loss 0.0003497021605653572\n",
      "epoch 227, batch 20, current loss 0.00033107379281318574\n",
      "epoch 227, batch 30, current loss 0.00022866421859362162\n",
      "epoch 228, batch 10, current loss 0.0011759741687455971\n",
      "epoch 228, batch 20, current loss 0.00014942333891667658\n",
      "epoch 228, batch 30, current loss 0.00031549356081086446\n",
      "epoch 229, batch 10, current loss 0.0017199133304529822\n",
      "epoch 229, batch 20, current loss 0.00026166048273807975\n",
      "epoch 229, batch 30, current loss 0.0005877226897609944\n",
      "epoch 230, batch 10, current loss 0.00032364594949285674\n",
      "epoch 230, batch 20, current loss 0.0006033588375430554\n",
      "epoch 230, batch 30, current loss 0.00028429720723579523\n",
      "epoch 230 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 230 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 231, batch 10, current loss 0.0020353716338831875\n",
      "epoch 231, batch 20, current loss 0.001047160829148197\n",
      "epoch 231, batch 30, current loss 0.00015047461947688135\n",
      "epoch 232, batch 10, current loss 0.00021164741592656356\n",
      "epoch 232, batch 20, current loss 0.004323048542573815\n",
      "epoch 232, batch 30, current loss 0.0008280077425297349\n",
      "epoch 233, batch 10, current loss 0.0012561208539409563\n",
      "epoch 233, batch 20, current loss 0.0003249232992857287\n",
      "epoch 233, batch 30, current loss 0.00010341534798499196\n",
      "epoch 234, batch 10, current loss 0.0009275486603655736\n",
      "epoch 234, batch 20, current loss 0.0009540612498540213\n",
      "epoch 234, batch 30, current loss 0.00010377238586443128\n",
      "epoch 235, batch 10, current loss 9.975542989195673e-05\n",
      "epoch 235, batch 20, current loss 0.0003584266657071566\n",
      "epoch 235, batch 30, current loss 0.0030742443798772003\n",
      "epoch 236, batch 10, current loss 0.00016747555246183766\n",
      "epoch 236, batch 20, current loss 0.0010451640941937512\n",
      "epoch 236, batch 30, current loss 0.0006216032451447972\n",
      "epoch 237, batch 10, current loss 0.0003239388588553993\n",
      "epoch 237, batch 20, current loss 0.00011093549128418089\n",
      "epoch 237, batch 30, current loss 0.00017738526103130425\n",
      "epoch 238, batch 10, current loss 0.00021715180037062964\n",
      "epoch 238, batch 20, current loss 0.00021028699857197352\n",
      "epoch 238, batch 30, current loss 0.00012253782215339016\n",
      "epoch 239, batch 10, current loss 9.78980691797915e-05\n",
      "epoch 239, batch 20, current loss 0.0025700891590531684\n",
      "epoch 239, batch 30, current loss 9.201643338201393e-05\n",
      "epoch 240, batch 10, current loss 0.0020389298828376924\n",
      "epoch 240, batch 20, current loss 0.00010733593508120976\n",
      "epoch 240, batch 30, current loss 0.0001258425040759903\n",
      "epoch 240 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 240 test accuracy: \n",
      "total_num:96, test accuracy:0.9583333333333334, positive_acc:0.9743589743589743, negative_acc:0.8888888888888888\n",
      "[[ 16.   2.]\n",
      " [  2.  76.]]\n",
      "[0, 90]\n",
      "[32, 38]\n",
      "epoch 241, batch 10, current loss 0.0005878471562027699\n",
      "epoch 241, batch 20, current loss 0.0003745783575141104\n",
      "epoch 241, batch 30, current loss 0.00022358313690347132\n",
      "epoch 242, batch 10, current loss 0.0001233205703101703\n",
      "epoch 242, batch 20, current loss 0.00024093734191410476\n",
      "epoch 242, batch 30, current loss 0.0002560995075782557\n",
      "epoch 243, batch 10, current loss 8.365153407794424e-05\n",
      "epoch 243, batch 20, current loss 0.0002569758948538947\n",
      "epoch 243, batch 30, current loss 0.0004235656986566028\n",
      "epoch 244, batch 10, current loss 0.0005559908076975262\n",
      "epoch 244, batch 20, current loss 0.0012093059660401196\n",
      "epoch 244, batch 30, current loss 0.00019275186541563017\n",
      "epoch 245, batch 10, current loss 0.00013233569407020696\n",
      "epoch 245, batch 20, current loss 0.0005483795818690851\n",
      "epoch 245, batch 30, current loss 0.00012593046194524503\n",
      "epoch 246, batch 10, current loss 7.064316469040932e-05\n",
      "epoch 246, batch 20, current loss 0.0013960193749881001\n",
      "epoch 246, batch 30, current loss 0.00015761750682941056\n",
      "epoch 247, batch 10, current loss 0.00032757060707808703\n",
      "epoch 247, batch 20, current loss 0.00037289142687768617\n",
      "epoch 247, batch 30, current loss 0.0001715546503874066\n",
      "epoch 248, batch 10, current loss 0.00035491273383740917\n",
      "epoch 248, batch 20, current loss 8.809758801362478e-05\n",
      "epoch 248, batch 30, current loss 0.0013621200425404823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 249, batch 10, current loss 0.00024165689537767321\n",
      "epoch 249, batch 20, current loss 0.06753499287942759\n",
      "epoch 249, batch 30, current loss 0.0003938825509976596\n",
      "epoch 250, batch 10, current loss 0.0020579210311552744\n",
      "epoch 250, batch 20, current loss 0.0015053903483931209\n",
      "epoch 250, batch 30, current loss 0.00044351248907332775\n",
      "epoch 250 train accuracy: \n",
      "total_num:471, test accuracy:1.0, positive_acc:1.0, negative_acc:1.0\n",
      "[[  86.    0.]\n",
      " [   0.  385.]]\n",
      "[]\n",
      "[]\n",
      "-------------------\n",
      "epoch 250 test accuracy: \n",
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9487179487179487, negative_acc:0.9444444444444444\n",
      "[[ 17.   1.]\n",
      " [  4.  74.]]\n",
      "[90]\n",
      "[16, 32, 37, 38]\n",
      "epoch 251, batch 10, current loss 0.00037665629461116625\n",
      "epoch 251, batch 20, current loss 0.0014377789422724164\n",
      "epoch 251, batch 30, current loss 4.813689292859635e-05\n",
      "epoch 252, batch 10, current loss 0.0003581197485800658\n",
      "epoch 252, batch 20, current loss 0.00038807898490631485\n",
      "epoch 252, batch 30, current loss 0.0019969898012277554\n",
      "epoch 253, batch 10, current loss 0.0001650109880756645\n",
      "epoch 253, batch 20, current loss 0.000565017971166526\n",
      "epoch 253, batch 30, current loss 0.0004977744702046039\n",
      "epoch 254, batch 10, current loss 0.0008479410658765119\n",
      "epoch 254, batch 20, current loss 0.0005348930193576962\n",
      "epoch 254, batch 30, current loss 0.00023888087744126097\n",
      "epoch 255, batch 10, current loss 0.0011751437463317417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1219:\n",
      "Process Process-1218:\n",
      "Process Process-1220:\n",
      "Process Process-1217:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2b75f5eee358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 119566) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6297964f18de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mMouse_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMouse_sub_volumes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMouse_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {} train accuracy: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c1b4edbd32c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mi_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_net()\n",
    "#net.apply(weight_init)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "print(\"There are {} parameters in the model\".format(count_parameters(net)))\n",
    "\n",
    "num_epochs = 350\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([3.5,1.0]).to(device))\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.00001)\n",
    "print('choose SGD as optimizer')\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr*10, weight_decay=0.00001)\n",
    "#print('choose Adam as optimizer')\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    \n",
    "    Mouse_dataset = Mouse_sub_volumes(train_data, transform=transforms.Compose([Flip()]))\n",
    "    dataloader = DataLoader(Mouse_dataset, batch_size=12, shuffle=True, num_workers=4, drop_last = True)\n",
    "    train(net, device, dataloader, optimizer, criterion, epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch {} train accuracy: '.format(epoch+1))\n",
    "        train_Mouse_dataset = Mouse_sub_volumes(train_data)\n",
    "        train_dataloader = DataLoader(train_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        train_dic = test(net, device, train_dataloader)\n",
    "        get_confusion_matrix(train_dic)\n",
    "        \n",
    "        print(\"-------------------\")\n",
    "        print('epoch {} test accuracy: '.format(epoch+1))\n",
    "        test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "        test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        test_dic = test(net, device, test_dataloader)\n",
    "        get_confusion_matrix(test_dic)\n",
    "        \n",
    "        torch.save(net.state_dict(), './model/mut_clas_2019_01_21_e{}_global2.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 495374 parameters in the model\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_net()\n",
    "#net.apply(weight_init)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "print(\"There are {} parameters in the model\".format(count_parameters(net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_probability(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    \n",
    "    true_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "            inputs = inputs.to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            outputs = F.softmax(outputs)\n",
    "            true_predicted_labels.append((labels.numpy(), predicted.cpu().numpy(), outputs.cpu().numpy()[0,0], outputs.cpu().numpy()[0,1]))\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    return true_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), './model/mut_clas_2019_01_15.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./model/mut_clas_2019_01_21_e250_global2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train accuracy: ')\n",
    "# Mouse_dataset = Mouse_sub_volumes(all_train_data,train_idx)\n",
    "# train_dataloader = DataLoader(Mouse_dataset, batch_size=128,\n",
    "#                         shuffle=False, num_workers=4)\n",
    "# test(net, device, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zq415/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num:96, test accuracy:0.9479166666666666, positive_acc:0.9487179487179487, negative_acc:0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy: ')\n",
    "Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "test_dataloader = DataLoader(Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dic = test_with_probability(net, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17.   1.]\n",
      " [  4.  74.]]\n",
      "[90]\n",
      "[16, 32, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "cross_table = np.zeros([2,2])\n",
    "mut_to_nor = []\n",
    "nor_to_mul = []\n",
    "\n",
    "for i in range(len(test_dic)):\n",
    "    if test_dic[i][0] ==0 and test_dic[i][1] ==0:\n",
    "        cross_table[0,0] += 1\n",
    "    elif  test_dic[i][0] ==0 and test_dic[i][1] ==1:\n",
    "        cross_table[0,1] += 1\n",
    "        mut_to_nor.append(i)\n",
    "    elif test_dic[i][0] ==1 and test_dic[i][1] ==0:\n",
    "        cross_table[1,0] += 1\n",
    "        nor_to_mul.append(i)\n",
    "    elif test_dic[i][0] ==1 and test_dic[i][1] ==1:\n",
    "        cross_table[1,1] += 1\n",
    "print(cross_table)\n",
    "print(mut_to_nor)\n",
    "print(nor_to_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0]), array([0]), 0.90518457, 0.094815448),\n",
       " (array([1]), array([1]), 6.5103113e-06, 0.99999344),\n",
       " (array([1]), array([1]), 8.3632345e-09, 1.0),\n",
       " (array([1]), array([1]), 3.5636092e-06, 0.99999642),\n",
       " (array([1]), array([1]), 3.7583898e-06, 0.99999619),\n",
       " (array([1]), array([1]), 2.0452902e-08, 1.0),\n",
       " (array([1]), array([1]), 4.933419e-09, 1.0),\n",
       " (array([1]), array([1]), 4.1862616e-08, 1.0),\n",
       " (array([1]), array([1]), 9.2822479e-11, 1.0),\n",
       " (array([0]), array([0]), 0.97405595, 0.025944086),\n",
       " (array([1]), array([1]), 2.2592277e-12, 1.0),\n",
       " (array([1]), array([1]), 1.9051833e-05, 0.99998093),\n",
       " (array([1]), array([1]), 1.0120527e-05, 0.99998987),\n",
       " (array([1]), array([1]), 4.7869928e-08, 1.0),\n",
       " (array([1]), array([1]), 1.0308761e-07, 0.99999988),\n",
       " (array([1]), array([1]), 1.0540753e-07, 0.99999988),\n",
       " (array([1]), array([0]), 0.55049348, 0.44950652),\n",
       " (array([1]), array([1]), 3.1962605e-07, 0.99999964),\n",
       " (array([1]), array([1]), 0.00016167587, 0.99983835),\n",
       " (array([1]), array([1]), 1.3782642e-06, 0.99999857),\n",
       " (array([1]), array([1]), 1.103116e-07, 0.99999988),\n",
       " (array([1]), array([1]), 0.00011587614, 0.99988413),\n",
       " (array([1]), array([1]), 1.5420656e-07, 0.99999988),\n",
       " (array([1]), array([1]), 5.4073912e-08, 1.0),\n",
       " (array([1]), array([1]), 4.1129046e-05, 0.99995887),\n",
       " (array([1]), array([1]), 3.2073422e-09, 1.0),\n",
       " (array([1]), array([1]), 1.6266195e-05, 0.99998379),\n",
       " (array([1]), array([1]), 1.6493947e-05, 0.99998355),\n",
       " (array([0]), array([0]), 0.99934906, 0.00065087969),\n",
       " (array([1]), array([1]), 6.2083245e-07, 0.9999994),\n",
       " (array([1]), array([1]), 2.0090894e-05, 0.99997985),\n",
       " (array([1]), array([1]), 1.0074729e-05, 0.99998987),\n",
       " (array([1]), array([0]), 0.98749304, 0.01250702),\n",
       " (array([1]), array([1]), 1.4156608e-06, 0.99999857),\n",
       " (array([0]), array([0]), 0.99941874, 0.00058128667),\n",
       " (array([0]), array([0]), 0.9999789, 2.1136497e-05),\n",
       " (array([0]), array([0]), 0.99865592, 0.001344067),\n",
       " (array([1]), array([0]), 0.56538492, 0.43461505),\n",
       " (array([1]), array([0]), 0.99233222, 0.0076677683),\n",
       " (array([0]), array([0]), 0.97659189, 0.023408076),\n",
       " (array([1]), array([1]), 0.0074679488, 0.99253207),\n",
       " (array([1]), array([1]), 1.5718527e-08, 1.0),\n",
       " (array([1]), array([1]), 8.5244889e-10, 1.0),\n",
       " (array([1]), array([1]), 0.00044170467, 0.99955827),\n",
       " (array([0]), array([0]), 0.99863189, 0.0013680297),\n",
       " (array([1]), array([1]), 0.26432028, 0.73567969),\n",
       " (array([1]), array([1]), 0.069968581, 0.93003142),\n",
       " (array([1]), array([1]), 1.3191558e-07, 0.99999988),\n",
       " (array([1]), array([1]), 1.5521413e-05, 0.9999845),\n",
       " (array([1]), array([1]), 8.0840813e-08, 0.99999988),\n",
       " (array([1]), array([1]), 7.6835171e-08, 0.99999988),\n",
       " (array([1]), array([1]), 6.3030719e-05, 0.99993694),\n",
       " (array([1]), array([1]), 1.1721637e-08, 1.0),\n",
       " (array([1]), array([1]), 1.9464567e-06, 0.99999809),\n",
       " (array([1]), array([1]), 1.0131685e-07, 0.99999988),\n",
       " (array([1]), array([1]), 2.102545e-08, 1.0),\n",
       " (array([1]), array([1]), 4.3334629e-09, 1.0),\n",
       " (array([1]), array([1]), 0.0110958, 0.98890418),\n",
       " (array([1]), array([1]), 1.4295207e-06, 0.99999857),\n",
       " (array([1]), array([1]), 1.3065843e-06, 0.99999869),\n",
       " (array([0]), array([0]), 0.99997461, 2.540622e-05),\n",
       " (array([0]), array([0]), 0.99999368, 6.2981971e-06),\n",
       " (array([0]), array([0]), 0.99991465, 8.5377105e-05),\n",
       " (array([0]), array([0]), 0.99999702, 3.0156164e-06),\n",
       " (array([1]), array([1]), 1.8084267e-05, 0.99998188),\n",
       " (array([1]), array([1]), 7.4763392e-08, 0.99999988),\n",
       " (array([0]), array([0]), 0.99970585, 0.0002941307),\n",
       " (array([1]), array([1]), 0.0043101162, 0.99568993),\n",
       " (array([1]), array([1]), 1.6386079e-07, 0.99999988),\n",
       " (array([1]), array([1]), 1.0800888e-08, 1.0),\n",
       " (array([1]), array([1]), 7.8918129e-06, 0.99999213),\n",
       " (array([1]), array([1]), 1.496688e-09, 1.0),\n",
       " (array([1]), array([1]), 7.1902919e-09, 1.0),\n",
       " (array([1]), array([1]), 0.0393898, 0.96061015),\n",
       " (array([0]), array([0]), 0.99933499, 0.00066499086),\n",
       " (array([1]), array([1]), 1.8428052e-06, 0.99999821),\n",
       " (array([1]), array([1]), 2.3639066e-06, 0.99999762),\n",
       " (array([1]), array([1]), 7.8529672e-07, 0.99999917),\n",
       " (array([0]), array([0]), 0.94266987, 0.057330158),\n",
       " (array([1]), array([1]), 7.7201969e-07, 0.99999928),\n",
       " (array([1]), array([1]), 7.2563466e-06, 0.99999273),\n",
       " (array([1]), array([1]), 3.5919587e-08, 1.0),\n",
       " (array([1]), array([1]), 4.221862e-09, 1.0),\n",
       " (array([1]), array([1]), 3.4072976e-08, 1.0),\n",
       " (array([1]), array([1]), 1.8081861e-06, 0.99999821),\n",
       " (array([1]), array([1]), 1.6492956e-07, 0.99999988),\n",
       " (array([0]), array([0]), 0.99899822, 0.0010017251),\n",
       " (array([1]), array([1]), 6.6124727e-08, 0.99999988),\n",
       " (array([0]), array([0]), 0.91048163, 0.089518309),\n",
       " (array([1]), array([1]), 6.9590042e-06, 0.99999309),\n",
       " (array([0]), array([1]), 0.00070192054, 0.9992981),\n",
       " (array([1]), array([1]), 4.8902848e-07, 0.99999952),\n",
       " (array([1]), array([1]), 3.9076934e-07, 0.99999964),\n",
       " (array([1]), array([1]), 0.0012225658, 0.99877745),\n",
       " (array([1]), array([1]), 5.945306e-07, 0.9999994),\n",
       " (array([1]), array([1]), 1.6339187e-09, 1.0)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in mut_to_nor:\n",
    "    print(i)\n",
    "    img_nft = nib.Nifti1Image(np.squeeze(test_data[i][0]+0.5),np.eye(4))\n",
    "    img_save_data_path = './img/mul_img{}_cam.nii'.format(i)\n",
    "    nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "32\n",
      "37\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "for i in nor_to_mul:\n",
    "    print(i)\n",
    "    img_nft = nib.Nifti1Image(np.squeeze(test_data[i][0]+0.5),np.eye(4))\n",
    "    img_save_data_path = './img/nor_img{}_cam.nii'.format(i)\n",
    "    nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if test_data[i][1] == 0:\n",
    "        img_nft = nib.Nifti1Image(np.squeeze(test_data[i][0]+0.5),np.eye(4))\n",
    "        img_save_data_path = './img/mul_img{}.nii'.format(i)\n",
    "        nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    ##############################################################################\n",
    "    # Perform a forward and backward pass through the model to compute the gradient \n",
    "    # of the correct class score with respect to each input image. You first want \n",
    "    # to compute the loss over the correct scores (we'll combine losses across a batch\n",
    "    # by summing), and then compute the gradients with a backward pass.\n",
    "    ##############################################################################\n",
    "    scores = model(X)\n",
    "    \n",
    "    # Get the correct class computed scores.\n",
    "    scores = scores.gather(1, y.view(-1, 1)).squeeze()  \n",
    "    \n",
    "    # Backward pass, need to supply initial gradients of same tensor shape as scores.\n",
    "    scores.backward(torch.tensor(10.0).cuda(device))\n",
    "    \n",
    "    # Get gradient for image.\n",
    "    saliency = X.grad.data\n",
    "    \n",
    "    # Convert from 3d to 1d.\n",
    "    saliency = saliency.abs()\n",
    "    saliency = saliency.squeeze()\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "    inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    saliency = compute_saliency_maps(inputs, labels, net)\n",
    "    \n",
    "    max_value = torch.max(saliency)\n",
    "    saliency[saliency >= (max_value*0.2)] = 1\n",
    "    saliency[saliency < (max_value*0.2)] = 0\n",
    "    \n",
    "    img_nft = nib.Nifti1Image(np.squeeze(inputs.cpu().detach().numpy()+0.5),np.eye(4))\n",
    "    img_save_data_path = './saliency_map/img_label{}_{}.nii'.format(labels.cpu().numpy()[0], i_batch)\n",
    "    nib.save(img_nft,img_save_data_path)\n",
    "    \n",
    "    saliency_nft = nib.Nifti1Image(np.squeeze(saliency.cpu().numpy()),np.eye(4))\n",
    "    saliency_save_data_path = './saliency_map/salency_label{}_{}.nii'.format(labels.cpu().numpy()[0], i_batch)\n",
    "    nib.save(saliency_nft,saliency_save_data_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(saliency).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = torch.max(saliency)\n",
    "saliency[saliency >= (max_value*0.2)] = 1\n",
    "saliency[saliency < (max_value*0.2)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 96])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (cell_name, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"cell_name\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "# outputs= []\n",
    "# def hook(module, input, output):\n",
    "#     outputs.append(output)\n",
    "\n",
    "# net.conv8_bn.register_forward_hook(hook)\n",
    "# out = net(res)\n",
    "# out = net(res1)\n",
    "# print(outputs)\n",
    "\n",
    "\n",
    "# test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "# test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "#     inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     saliency = compute_saliency_maps(inputs, labels, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "net.eval()\n",
    "\n",
    "fc_weight = net.fc1.weight.data\n",
    "\n",
    "res50_conv = nn.Sequential(*list(net.children())[:-3])\n",
    "for param in res50_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "    inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "    saliency = compute_cam_maps(inputs, labels, net, fc_weight, res50_conv)\n",
    "    \n",
    "#     max_value = np.max(saliency)\n",
    "#     saliency[saliency >= (max_value*0.2)] = 1\n",
    "#     saliency[saliency < (max_value*0.2)] = 0\n",
    "    \n",
    "    img_nft = nib.Nifti1Image(np.squeeze(inputs.numpy()+0.5),np.eye(4))\n",
    "    img_save_data_path = './cam_map/img_label{}_{}.nii'.format(labels.numpy()[0], i_batch)\n",
    "    nib.save(img_nft,img_save_data_path)\n",
    "    \n",
    "    saliency_nft = nib.Nifti1Image(np.squeeze(saliency),np.eye(4))\n",
    "    saliency_save_data_path = './cam_map/salency_label{}_{}.nii'.format(labels.numpy()[0], i_batch)\n",
    "    nib.save(saliency_nft,saliency_save_data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213477.34"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam represent class saliency map\n",
    "def compute_cam_maps(X, y, model, fc_weight, feature_extract): \n",
    "    model.eval()\n",
    "    \n",
    "    outputs = feature_extract(X).squeeze()\n",
    "    channels = outputs.shape[0]\n",
    "    saliency = outputs[0,...] * fc_weight[y, 0]\n",
    "    for i in range(1,channels):\n",
    "        saliency += outputs[i,...] * fc_weight[y, i]\n",
    "    saliency = zoom(saliency.numpy(), 8)\n",
    "    \n",
    "    saliency = saliency - np.min(saliency)\n",
    "    saliency = saliency / np.max(saliency)\n",
    "    \n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,96):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n",
      "torch.Size([1, 96, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "net.cpu()\n",
    "net.eval()\n",
    "\n",
    "fc_weight = net.fc1.weight.data\n",
    "\n",
    "res50_conv = nn.Sequential(*list(net.children())[:-2])\n",
    "for param in res50_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "    inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "    print(res50_conv(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
