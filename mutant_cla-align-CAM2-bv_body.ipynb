{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "import random\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchcontrib.optim import SWA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def read_mutant_txt(path):\n",
    "    name_list = []\n",
    "    fo = open(path)\n",
    "    for line in fo:\n",
    "        striped_line = line.strip('\\n')\n",
    "        if striped_line != '':\n",
    "            name_list.append(striped_line)\n",
    "    return name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutant_names = read_mutant_txt('mutant_imgs.txt')\n",
    "data_base_path = '/scratch/zq415/grammar_cor/mutant_detect/mutant_cla/data'\n",
    "data_nii_path = '/scratch/zq415/grammar_cor/mutant_detect/mutant_cla/bv_body_nii'\n",
    "save_name = 'All_data_256_196_160_down.pickle'#'All_data_256_196_160.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(data_base_path,save_name)):\n",
    "    with open(os.path.join(data_base_path, save_name), \"rb\") as input_file:\n",
    "        all_train_data = pickle.load(input_file)\n",
    "        \n",
    "else:\n",
    "    files = glob.glob(data_nii_path + '/*.nii')\n",
    "    all_train_data = {}\n",
    "    for i in tqdm(range(len(files))):\n",
    "        base_name = os.path.basename(files[i])[:-17]\n",
    "        label = nib.load(files[i])\n",
    "        ori_label = np.uint8(label.get_data()[::2,::2,::2])\n",
    "        print(ori_label.shape)\n",
    "        all_train_data[base_name] = ori_label\n",
    "    if not os.path.exists(os.path.join(data_base_path,save_name)):\n",
    "        save_file = open(os.path.join(data_base_path, save_name),'wb')\n",
    "        pickle.dump(all_train_data,save_file)\n",
    "        save_file.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ['20170207_En1_E13_E12a_Mut', '20170207_En1_E13_E12b_Mut', '20170207_En1_E13_E12c_Mut', '20170207_En1_E13_E12d_Mut', '20170727_En1_E13_E1a_Mut', '20170727_En1_E13_E1b_Mut', '20161222_En1_E13_E1a_Mut', '20161222_En1_E13_E1b_Mut', '20161222_En1_E13_E1c_Mut', '20180202_En1_E14_E4a_mut', '20180202_En1_E14_E4b_Mut', '20171211_En1_E10_E4b', '20170705_En1_E12_E1a_filt', '20171004_En1_E13_E5b_Mut', '20171009_En1_E12_E11a', '20161219_En1_E10_E5Ma', '20161223_En1_E14_E1a_Mut']\n"
     ]
    }
   ],
   "source": [
    "mutant_group = [(9,10), (12,13,14), (16,17,18,19), (36,37), (38,39), (42,43), (44,45,46,47), (48,49,50,51), (52,53), (54,55),\n",
    "(56,57,58), (59,60,61), (63,64), (66,67,68), (69,70,71), (73,74), (75,76), (77,78,79), (80,81,82,83,84,85,86,87),\n",
    "(89,90), (91,92), (93,94), (95,96,97), (98,99), (100,101,102)]\n",
    "\n",
    "group_list = []\n",
    "for one_group in mutant_group:\n",
    "    for ii in range(len(one_group)):\n",
    "        group_list.append(one_group[ii])\n",
    "single_mutant = [i for i in range(len(mutant_names)) if i not in group_list]\n",
    "\n",
    "test_mut_names = []\n",
    "for i in range(2,len(mutant_group),6):\n",
    "    for ii in range(len(mutant_group[i])):\n",
    "        test_mut_names.append(mutant_names[mutant_group[i][ii]])\n",
    "\n",
    "for i in range(2,len(single_mutant),6):\n",
    "    test_mut_names.append(mutant_names[single_mutant[i]])\n",
    "    \n",
    "print(len(test_mut_names),test_mut_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for bv only\n",
    "# for i_key in all_train_data:\n",
    "#     all_train_data[i_key] = np.uint8(all_train_data[i_key] > 1.5)\n",
    "    \n",
    "# train_data = []\n",
    "# test_data = []\n",
    "# seed_num = 0\n",
    "\n",
    "# for name in all_train_data:\n",
    "#     if name in mutant_names:\n",
    "#         if name in test_mut_names:\n",
    "#             test_data.append((all_train_data[name]-0.5, 0))\n",
    "#             print('mutant: ',name)\n",
    "#         else:\n",
    "#             train_data.append((all_train_data[name]-0.5, 0))\n",
    "#     else:\n",
    "#         random.seed(seed_num*8)\n",
    "# #         random.seed(seed_num*9)\n",
    "#         seed_num += 1\n",
    "#         if random.uniform(0,1) < 0.16:\n",
    "#             test_data.append((all_train_data[name]-0.5, 1))\n",
    "#             print('normal: ',name)\n",
    "#         else:\n",
    "#             train_data.append((all_train_data[name]-0.5, 1))\n",
    "\n",
    "# print(len(test_data))\n",
    "# print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for body only\n",
    "# for i_key in all_train_data:\n",
    "#     all_train_data[i_key] = np.uint8(all_train_data[i_key] > 0.5)\n",
    "    \n",
    "# train_data = []\n",
    "# test_data = []\n",
    "# seed_num = 0\n",
    "\n",
    "# for name in all_train_data:\n",
    "#     if name in mutant_names:\n",
    "#         if name in test_mut_names:\n",
    "#             test_data.append((all_train_data[name]-0.5, 0))\n",
    "#             print('mutant: ',name)\n",
    "#         else:\n",
    "#             train_data.append((all_train_data[name]-0.5, 0))\n",
    "#     else:\n",
    "#         random.seed(seed_num*8)\n",
    "# #         random.seed(seed_num*9)\n",
    "#         seed_num += 1\n",
    "#         if random.uniform(0,1) < 0.16:\n",
    "#             test_data.append((all_train_data[name]-0.5, 1))\n",
    "#             print('normal: ',name)\n",
    "#         else:\n",
    "#             train_data.append((all_train_data[name]-0.5, 1))\n",
    "\n",
    "# print(len(test_data))\n",
    "# print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 440\n"
     ]
    }
   ],
   "source": [
    "mutant_num = 0\n",
    "normal_num = 0\n",
    "\n",
    "for name in all_train_data:\n",
    "    if name in mutant_names:\n",
    "        mutant_num += 1\n",
    "    else:\n",
    "        normal_num += 1\n",
    "        \n",
    "print(mutant_num, normal_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal:  20180228_En1_E13p5_Ext_E1a\n",
      "normal:  20180302_En1_E14.5_Ext_E11a_reg\n",
      "normal:  20171212_En1_E11_E8a\n",
      "normal:  20180202_En1_E14_E12a\n",
      "normal:  20171003_En1_E13_E8a_reg\n",
      "normal:  20170129_En1_E11_E2a_reg\n",
      "normal:  20170206_En1_E12_E3a\n",
      "normal:  20170320_En1_E11_E6a\n",
      "normal:  20180205_En1_E11_E1c_reg\n",
      "mutant:  20180202_En1_E14_E4a_mut\n",
      "normal:  20180302_En1_E14.5_Ext_E8a_reg\n",
      "normal:  20180904_En1_E11p5_4a_007\n",
      "mutant:  20170727_En1_E13_E1a_Mut\n",
      "normal:  20170313_EN1_E10_E3b_reg_filt\n",
      "normal:  20161207_En1_E14_E4a\n",
      "normal:  20170619_En1_E12_E2a\n",
      "normal:  20180202_En1_E14_E1a\n",
      "normal:  20180226_En1_E11p5_Em2a\n",
      "normal:  20161116_En1_E12_E6a_reg\n",
      "normal:  20161222_En1_E13_E2a\n",
      "normal:  20171002_En1_E11_E1a\n",
      "normal:  20170315_En1_E12_E2b\n",
      "mutant:  20161222_En1_E13_E1c_Mut\n",
      "normal:  20170331_En1_E14p_E2a\n",
      "normal:  20170705_En1_E12_E5a_filt\n",
      "normal:  20161116_En1_E12_E2a_reg\n",
      "normal:  20170619_En1_E12_Ex_E12a_reg\n",
      "normal:  20180226_En1_E11p5_Em1a\n",
      "mutant:  20170207_En1_E13_E12a_Mut\n",
      "normal:  20170314_En1_E11_E4b\n",
      "mutant:  20161222_En1_E13_E1b_Mut\n",
      "normal:  20180202_En1_E14_E7a_reg\n",
      "normal:  20170301_En1_E12_E1b_filt\n",
      "normal:  20180302_En1_E14.5_Ext_E1a\n",
      "normal:  20180228_En1_E13p5_Ext_E4a_reg\n",
      "normal:  20170330_En1_E13_E2a\n",
      "normal:  20171009_En1_E12_E3a\n",
      "normal:  20180207_En1_E13_E1b\n",
      "normal:  20180228_En1_E13p5_Ext_E7a_reg\n",
      "normal:  20180302_En1_E14.5_Em2a\n",
      "mutant:  20171004_En1_E13_E5b_Mut\n",
      "normal:  20170706_En1_E13_E7a_reg\n",
      "normal:  20170316_En1_E13_E3a\n",
      "normal:  20180905_EN1_E14p5_Ext_E5a_027\n",
      "normal:  20170314_En1_E11_E2b\n",
      "normal:  20171003_En1_E13_E3a_reg\n",
      "normal:  20180307_En1_E14p5_M1_Em1a\n",
      "normal:  20180201_En1_M2_E13_E5a\n",
      "mutant:  20170207_En1_E13_E12c_Mut\n",
      "normal:  20180905_EN1_E14p5_E3a_020\n",
      "normal:  20170129_En1_E11_E3a_reg\n",
      "normal:  20171002_En1_E11_E4a\n",
      "normal:  20161115_En1_E11_E1a_reg\n",
      "normal:  20171003_En1_E13_E5c_reg\n",
      "normal:  20170718_En1_E12_E4a_reg\n",
      "normal:  20180228_En1_E13p5_Ext_E7a\n",
      "normal:  20180228_En1_E13p5_Ext_E11a_reg\n",
      "normal:  20170626_En1_E13p5_E5a\n",
      "mutant:  20161222_En1_E13_E1a_Mut\n",
      "mutant:  20161219_En1_E10_E5Ma\n",
      "normal:  20180130_En1_E11p5_E1a_reg\n",
      "mutant:  20170207_En1_E13_E12b_Mut\n",
      "normal:  20170206_En1_E12_E4a\n",
      "normal:  20171003_En1_E13_E10a_reg\n",
      "normal:  20170718_En1_E12_E3a_reg\n",
      "normal:  20180307_En1_E14p5_M1_Ext_Em16a\n",
      "normal:  20180205_En1_E11_E1c\n",
      "normal:  20170327_En1_E10_E4a\n",
      "mutant:  20180202_En1_E14_E4b_Mut\n",
      "normal:  20180228_En1_E13p5_Ext_E3a\n",
      "normal:  20170301_En1_E12_E3a_filt\n",
      "normal:  20171002_En1_E11_E3a\n",
      "normal:  20180228_En1_E13p5_E5a_reg\n",
      "normal:  20180130_En1_E12p5_E2b_reg\n",
      "normal:  20180306_En1_E13p5_M1_Em3a\n",
      "mutant:  20161223_En1_E14_E1a_Mut\n",
      "normal:  20161128_En1_E14_E1R-filt\n",
      "normal:  20170706_En1_E13_E8a_reg\n",
      "mutant:  20171211_En1_E10_E4b\n",
      "mutant:  20170207_En1_E13_E12d_Mut\n",
      "normal:  20180905_EN1_E14p5_Ext_E1a_023\n",
      "normal:  20180226_En1_E10p5_Em2a\n",
      "normal:  20161128_En1_E14_E2b-filt\n",
      "normal:  20161121_En1_E3a_reg-filt\n",
      "mutant:  20170705_En1_E12_E1a_filt\n",
      "normal:  20180201_En1_M2_E13_E3b\n",
      "normal:  20180202_En1_E14_E8a\n",
      "mutant:  20170727_En1_E13_E1b_Mut\n",
      "normal:  20180205_En1_E11_E1b\n",
      "normal:  20180905_EN1_E14p5_Ext_E6a_028\n",
      "normal:  20161223_En1_E14_Ext_E1a\n",
      "normal:  20170706_En1_E13_E3a_reg\n",
      "mutant:  20171009_En1_E12_E11a\n",
      "93\n",
      "449\n"
     ]
    }
   ],
   "source": [
    "#####for bv and body\n",
    "train_data = []\n",
    "test_data = []\n",
    "seed_num = 0\n",
    "\n",
    "for name in all_train_data:\n",
    "    if name in mutant_names:\n",
    "        if name in test_mut_names:\n",
    "            test_data.append((all_train_data[name]-1.0, 0))\n",
    "            print('mutant: ',name)\n",
    "        else:\n",
    "            train_data.append((all_train_data[name]-1.0,0))\n",
    "    else:\n",
    "        random.seed(seed_num*8)\n",
    "#         random.seed(seed_num*9)\n",
    "        seed_num += 1\n",
    "        if random.uniform(0,1) < 0.16:\n",
    "            test_data.append((all_train_data[name]-1.0,1))\n",
    "            print('normal: ',name)\n",
    "        else:\n",
    "            train_data.append((all_train_data[name]-1.0,1))\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_nft = nib.Nifti1Image(np.squeeze(all_train_data[193]),np.eye(4))\n",
    "#img_save_data_path = './img/mul_img.nii'\n",
    "#nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Mouse_sub_volumes(Dataset):\n",
    "    \"\"\"Mouse sub-volumes BV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, all_data , transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            all_whole_volumes: Contain all the padded whole BV volumes as a dic\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.all_data = all_data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, num):\n",
    "        \n",
    "        current_img, label = self.all_data[num]\n",
    "        \n",
    "        img = np.float32(current_img[np.newaxis,...])\n",
    "        sample = {'image': img, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Flip the image for data augmentation, but prefer original image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,ori_probability=0.20):\n",
    "        self.ori_probability = ori_probability\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0,1) < self.ori_probability:\n",
    "            return sample\n",
    "        else:\n",
    "            img, label = sample['image'], sample['label']\n",
    "            random_choise1=random.choice([1,2,3,4,5,6,7,8])\n",
    "            img[0,...] = {1: lambda x: x,\n",
    "                          2: lambda x: x[::-1,:,:],\n",
    "                          3: lambda x: x[:,::-1,:],\n",
    "                          4: lambda x: x[:,:,::-1],\n",
    "                          5: lambda x: x[::-1,::-1,:],\n",
    "                          6: lambda x: x[::-1,:,::-1],\n",
    "                          7: lambda x: x[:,::-1,::-1],\n",
    "                          8: lambda x: x[::-1,::-1,::-1]\n",
    "                          }[random_choise1](img[0,...])\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class VGG_net(nn.Module):\n",
    "    def __init__(self,conv_drop_rate=0.10,linear_drop_rate=0.10):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=12, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_bn = nn.BatchNorm3d(12)\n",
    "        self.conv2 = nn.Conv3d(in_channels=12, out_channels=12, kernel_size=3,stride=1,padding=2, dilation=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2_bn = nn.BatchNorm3d(12)\n",
    "        self.pool1 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout1 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(in_channels=12, out_channels=24, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv3_bn = nn.BatchNorm3d(24)\n",
    "        self.conv4 = nn.Conv3d(in_channels=24, out_channels=24, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.conv4_bn = nn.BatchNorm3d(24)\n",
    "        self.pool2 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout2 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv5 = nn.Conv3d(in_channels=24, out_channels=48, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv5_bn = nn.BatchNorm3d(48)\n",
    "        self.conv6 = nn.Conv3d(in_channels=48, out_channels=48, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.conv6_bn = nn.BatchNorm3d(48)\n",
    "        self.pool3 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout3 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv7 = nn.Conv3d(in_channels=48, out_channels=72, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.conv7_bn = nn.BatchNorm3d(72)\n",
    "        self.conv8 = nn.Conv3d(in_channels=72, out_channels=72, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "        self.relu8 = nn.ReLU(inplace=True)\n",
    "        self.conv8_bn = nn.BatchNorm3d(72)\n",
    "        self.pool4 = nn.AdaptiveAvgPool3d((1,1,1))\n",
    "        self.dropout4 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(72, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1_bn(self.relu1(self.conv1(x)))\n",
    "        x = self.dropout1(self.pool1(self.conv2_bn(self.relu2(self.conv2(x)))))\n",
    "        \n",
    "        x = self.conv3_bn(self.relu3(self.conv3(x)))\n",
    "        x = self.dropout2(self.pool2(self.conv4_bn(self.relu4(self.conv4(x)))))\n",
    "        \n",
    "        x = self.conv5_bn(self.relu5(self.conv5(x)))\n",
    "        x = self.dropout3(self.pool3(self.conv6_bn(self.relu6(self.conv6(x)))))\n",
    "        \n",
    "        x = self.conv7_bn(self.relu7(self.conv7(x)))\n",
    "        x = self.conv8_bn(self.relu8(self.conv8(x)))\n",
    "        \n",
    "        x1 = self.dropout4(self.pool4(x))\n",
    "#         x2 = self.dropout5(self.pool5(x))\n",
    "        \n",
    "        x1 = x1.view(-1, 72)\n",
    "#         x2 = x2.view(-1, 72)\n",
    "#         x = torch.cat((x1, x2), 1)\n",
    "        x = x1\n",
    "        #x = self.dropout5(self.fc1_bn(F.relu(self.fc1(x))))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch\n",
    "\n",
    "# class VGG_net(nn.Module):\n",
    "#     def __init__(self,conv_drop_rate=0.10,linear_drop_rate=0.10):\n",
    "#         super(VGG_net, self).__init__()\n",
    "#         self.conv1 = nn.Conv3d(in_channels=1, out_channels=12, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv1_bn = nn.BatchNorm3d(12)\n",
    "#         self.conv2 = nn.Conv3d(in_channels=12, out_channels=12, kernel_size=3,stride=1,padding=2, dilation=2)\n",
    "#         self.conv2_bn = nn.BatchNorm3d(12)\n",
    "#         self.pool1 = nn.MaxPool3d(2, 2)\n",
    "#         self.dropout1 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "#         self.conv3 = nn.Conv3d(in_channels=12, out_channels=24, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv3_bn = nn.BatchNorm3d(24)\n",
    "#         self.conv4 = nn.Conv3d(in_channels=24, out_channels=24, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv4_bn = nn.BatchNorm3d(24)\n",
    "#         self.pool2 = nn.MaxPool3d(2, 2)\n",
    "#         self.dropout2 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "#         self.conv5 = nn.Conv3d(in_channels=24, out_channels=48, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv5_bn = nn.BatchNorm3d(48)\n",
    "#         self.conv6 = nn.Conv3d(in_channels=48, out_channels=48, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv6_bn = nn.BatchNorm3d(48)\n",
    "#         self.pool3 = nn.MaxPool3d(2, 2)\n",
    "#         self.dropout3 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "#         self.conv7 = nn.Conv3d(in_channels=48, out_channels=72, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv7_bn = nn.BatchNorm3d(72)\n",
    "#         self.conv8 = nn.Conv3d(in_channels=72, out_channels=72, kernel_size=3,stride=1, padding=2,dilation=2)\n",
    "#         self.conv8_bn = nn.BatchNorm3d(72)\n",
    "#         self.pool4 = nn.AdaptiveAvgPool3d((1,1,1))\n",
    "#         self.dropout4 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        \n",
    "#         self.fc1 = nn.Linear(72, 2)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1_bn(F.relu(self.conv1(x)))\n",
    "#         x = self.dropout1(self.pool1(self.conv2_bn(F.relu(self.conv2(x)))))\n",
    "        \n",
    "#         x = self.conv3_bn(F.relu(self.conv3(x)))\n",
    "#         x = self.dropout2(self.pool2(self.conv4_bn(F.relu(self.conv4(x)))))\n",
    "        \n",
    "#         x = self.conv5_bn(F.relu(self.conv5(x)))\n",
    "#         x = self.dropout3(self.pool3(self.conv6_bn(F.relu(self.conv6(x)))))\n",
    "        \n",
    "#         x = self.conv7_bn(F.relu(self.conv7(x)))\n",
    "#         x = self.conv8_bn(F.relu(self.conv8(x)))\n",
    "        \n",
    "#         x1 = self.dropout4(self.pool4(x))\n",
    "# #         x2 = self.dropout5(self.pool5(x))\n",
    "        \n",
    "#         x1 = x1.view(-1, 72)\n",
    "# #         x2 = x2.view(-1, 72)\n",
    "# #         x = torch.cat((x1, x2), 1)\n",
    "#         x = x1\n",
    "#         #x = self.dropout5(self.fc1_bn(F.relu(self.fc1(x))))\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        i_batch += 1\n",
    "        if i_batch % 10 == 0:\n",
    "            print(\"epoch {}, batch {}, current loss {}\".format(epoch+1,i_batch,running_loss/10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    \n",
    "    true_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "            inputs = inputs.to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_predicted_labels.append((labels.numpy(), predicted.cpu().numpy()))\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    return true_predicted_labels\n",
    "\n",
    "def get_confusion_matrix(true_predicted_labels):\n",
    "    cross_table = np.zeros([2,2])\n",
    "    mut_to_nor = []\n",
    "    nor_to_mul = []\n",
    "    test_dic = true_predicted_labels\n",
    "    for i in range(len(test_dic)):\n",
    "        if test_dic[i][0] ==0 and test_dic[i][1] ==0:\n",
    "            cross_table[0,0] += 1\n",
    "        elif  test_dic[i][0] ==0 and test_dic[i][1] ==1:\n",
    "            cross_table[0,1] += 1\n",
    "            mut_to_nor.append(i)\n",
    "        elif test_dic[i][0] ==1 and test_dic[i][1] ==0:\n",
    "            cross_table[1,0] += 1\n",
    "            nor_to_mul.append(i)\n",
    "        elif test_dic[i][0] ==1 and test_dic[i][1] ==1:\n",
    "            cross_table[1,1] += 1\n",
    "    print(cross_table)\n",
    "    print(mut_to_nor)\n",
    "    print(nor_to_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 355214 parameters in the model\n",
      "choose SGD as optimizer\n",
      "epoch 1, batch 10, current loss 0.6747636795043945\n",
      "epoch 1, batch 20, current loss 0.690155291557312\n",
      "epoch 1, batch 30, current loss 0.6850602984428406\n",
      "epoch 1, batch 40, current loss 0.6396723717451096\n",
      "epoch 1, batch 50, current loss 0.6886747300624847\n",
      "epoch 2, batch 10, current loss 0.635322916507721\n",
      "epoch 2, batch 20, current loss 0.6901465892791748\n",
      "epoch 2, batch 30, current loss 0.6325404733419419\n",
      "epoch 2, batch 40, current loss 0.610495176911354\n",
      "epoch 2, batch 50, current loss 0.7106066405773163\n",
      "epoch 3, batch 10, current loss 0.7380648553371429\n",
      "epoch 3, batch 20, current loss 0.6472429931163788\n",
      "epoch 3, batch 30, current loss 0.7002818435430527\n",
      "epoch 3, batch 40, current loss 0.6520844459533691\n",
      "epoch 3, batch 50, current loss 0.684912133216858\n",
      "epoch 4, batch 10, current loss 0.6847851395606994\n",
      "epoch 4, batch 20, current loss 0.6410349756479263\n",
      "epoch 4, batch 30, current loss 0.7217817723751068\n",
      "epoch 4, batch 40, current loss 0.5918744236230851\n",
      "epoch 4, batch 50, current loss 0.7329046905040741\n",
      "epoch 5, batch 10, current loss 0.6772463381290436\n",
      "epoch 5, batch 20, current loss 0.6247047424316406\n",
      "epoch 5, batch 30, current loss 0.6586428672075272\n",
      "epoch 5, batch 40, current loss 0.6916636884212494\n",
      "epoch 5, batch 50, current loss 0.6411413729190827\n",
      "epoch 6, batch 10, current loss 0.7138850063085556\n",
      "epoch 6, batch 20, current loss 0.62501200735569\n",
      "epoch 6, batch 30, current loss 0.6801110655069351\n",
      "epoch 6, batch 40, current loss 0.6462352693080902\n",
      "epoch 6, batch 50, current loss 0.625118362903595\n",
      "epoch 7, batch 10, current loss 0.7009122431278229\n",
      "epoch 7, batch 20, current loss 0.6803377509117127\n",
      "epoch 7, batch 30, current loss 0.6664742827415466\n",
      "epoch 7, batch 40, current loss 0.6608859479427338\n",
      "epoch 7, batch 50, current loss 0.5998160719871521\n",
      "epoch 8, batch 10, current loss 0.6780167192220687\n",
      "epoch 8, batch 20, current loss 0.691498064994812\n",
      "epoch 8, batch 30, current loss 0.6841763198375702\n",
      "epoch 8, batch 40, current loss 0.7296391665935517\n",
      "epoch 8, batch 50, current loss 0.6277725517749786\n",
      "epoch 9, batch 10, current loss 0.6814384967088699\n",
      "epoch 9, batch 20, current loss 0.6375225096940994\n",
      "epoch 9, batch 30, current loss 0.6155672043561935\n",
      "epoch 9, batch 40, current loss 0.5571457982063294\n",
      "epoch 9, batch 50, current loss 0.7259938955307007\n",
      "epoch 10, batch 10, current loss 0.5292656302452088\n",
      "epoch 10, batch 20, current loss 0.6650259792804718\n",
      "epoch 10, batch 30, current loss 0.6571404933929443\n",
      "epoch 10, batch 40, current loss 0.6795915216207504\n",
      "epoch 10, batch 50, current loss 0.5990847229957581\n",
      "epoch 10 train accuracy: \n",
      "total_num:449, test accuracy:0.8173719376391982, positive_acc:0.9972527472527473, negative_acc:0.047058823529411764\n",
      "[[   4.   81.]\n",
      " [   1.  363.]]\n",
      "[0, 4, 6, 8, 20, 23, 25, 37, 40, 45, 46, 52, 54, 59, 65, 70, 78, 88, 98, 112, 117, 118, 120, 129, 130, 136, 146, 151, 154, 156, 162, 177, 183, 201, 207, 212, 213, 230, 234, 244, 246, 250, 252, 253, 277, 285, 294, 302, 306, 310, 311, 312, 313, 315, 321, 322, 324, 326, 327, 328, 330, 342, 347, 349, 353, 355, 361, 365, 374, 382, 387, 389, 392, 394, 397, 405, 434, 437, 438, 441, 444]\n",
      "[169]\n",
      "-------------------\n",
      "epoch 10 test accuracy: \n",
      "total_num:93, test accuracy:0.8172043010752689, positive_acc:1.0, negative_acc:0.0\n",
      "[[  0.  17.]\n",
      " [  0.  76.]]\n",
      "[9, 12, 22, 28, 30, 40, 48, 58, 59, 61, 68, 75, 78, 79, 84, 87, 92]\n",
      "[]\n",
      "epoch 11, batch 10, current loss 0.6403593242168426\n",
      "epoch 11, batch 20, current loss 0.6620294928550721\n",
      "epoch 11, batch 30, current loss 0.6416720986366272\n",
      "epoch 11, batch 40, current loss 0.6838202357292176\n",
      "epoch 11, batch 50, current loss 0.6655081987380982\n",
      "epoch 12, batch 10, current loss 0.614497321844101\n",
      "epoch 12, batch 20, current loss 0.6442474752664566\n",
      "epoch 12, batch 30, current loss 0.6375656247138977\n",
      "epoch 12, batch 40, current loss 0.6492304384708405\n",
      "epoch 12, batch 50, current loss 0.5958846598863602\n",
      "epoch 13, batch 10, current loss 0.6269842565059662\n",
      "epoch 13, batch 20, current loss 0.5834435492753982\n",
      "epoch 13, batch 30, current loss 0.6183915555477142\n",
      "epoch 13, batch 40, current loss 0.4980964809656143\n",
      "epoch 13, batch 50, current loss 0.5717983320355415\n",
      "epoch 14, batch 10, current loss 0.6675906956195832\n",
      "epoch 14, batch 20, current loss 0.6290986031293869\n",
      "epoch 14, batch 30, current loss 0.5201604247093201\n",
      "epoch 14, batch 40, current loss 0.5470800220966339\n",
      "epoch 14, batch 50, current loss 0.5637589454650879\n",
      "epoch 15, batch 10, current loss 0.4803654506802559\n",
      "epoch 15, batch 20, current loss 0.6338835820555687\n",
      "epoch 15, batch 30, current loss 0.6059830754995346\n",
      "epoch 15, batch 40, current loss 0.6321139127016068\n",
      "epoch 15, batch 50, current loss 0.6410959124565124\n",
      "epoch 16, batch 10, current loss 0.5936882734298706\n",
      "epoch 16, batch 20, current loss 0.5905401051044464\n",
      "epoch 16, batch 30, current loss 0.693042802810669\n",
      "epoch 16, batch 40, current loss 0.6354003250598907\n",
      "epoch 16, batch 50, current loss 0.5326839357614517\n",
      "epoch 17, batch 10, current loss 0.5707650303840637\n",
      "epoch 17, batch 20, current loss 0.5262017011642456\n",
      "epoch 17, batch 30, current loss 0.47656848430633547\n",
      "epoch 17, batch 40, current loss 0.48202607184648516\n",
      "epoch 17, batch 50, current loss 0.5372141972184181\n",
      "epoch 18, batch 10, current loss 0.4383967235684395\n",
      "epoch 18, batch 20, current loss 0.45118638724088667\n",
      "epoch 18, batch 30, current loss 0.5556502416729927\n",
      "epoch 18, batch 40, current loss 0.4362954288721085\n",
      "epoch 18, batch 50, current loss 0.5071540459990501\n",
      "epoch 19, batch 10, current loss 0.44112311154603956\n",
      "epoch 19, batch 20, current loss 0.5617503389716149\n",
      "epoch 19, batch 30, current loss 0.5098341077566146\n",
      "epoch 19, batch 40, current loss 0.43354127556085587\n",
      "epoch 19, batch 50, current loss 0.4817492485046387\n",
      "epoch 20, batch 10, current loss 0.4013619408011436\n",
      "epoch 20, batch 20, current loss 0.3285075858235359\n",
      "epoch 20, batch 30, current loss 0.27092891186475754\n",
      "epoch 20, batch 40, current loss 0.5536917932331562\n",
      "epoch 20, batch 50, current loss 0.5362284868955612\n",
      "epoch 20 train accuracy: \n",
      "total_num:449, test accuracy:0.8864142538975501, positive_acc:0.9065934065934066, negative_acc:0.8\n",
      "[[  68.   17.]\n",
      " [  34.  330.]]\n",
      "[45, 129, 201, 213, 230, 234, 306, 310, 311, 315, 347, 349, 361, 387, 405, 438, 441]\n",
      "[1, 3, 14, 17, 29, 62, 81, 85, 137, 148, 153, 169, 199, 210, 227, 228, 236, 243, 247, 266, 286, 287, 309, 333, 340, 354, 373, 377, 386, 393, 403, 411, 435, 448]\n",
      "-------------------\n",
      "epoch 20 test accuracy: \n",
      "total_num:93, test accuracy:0.9247311827956989, positive_acc:0.9210526315789473, negative_acc:0.9411764705882353\n",
      "[[ 16.   1.]\n",
      " [  6.  70.]]\n",
      "[78]\n",
      "[5, 23, 25, 27, 83, 88]\n",
      "epoch 21, batch 10, current loss 0.42547830045223234\n",
      "epoch 21, batch 20, current loss 0.5718680985271931\n",
      "epoch 21, batch 30, current loss 0.4374420613050461\n",
      "epoch 21, batch 40, current loss 0.3779966220259666\n",
      "epoch 21, batch 50, current loss 0.3238977327942848\n",
      "epoch 22, batch 10, current loss 0.3678601160645485\n",
      "epoch 22, batch 20, current loss 0.36466012746095655\n",
      "epoch 22, batch 30, current loss 0.2595081150531769\n",
      "epoch 22, batch 40, current loss 0.4728077664971352\n",
      "epoch 22, batch 50, current loss 0.49143953174352645\n",
      "epoch 23, batch 10, current loss 0.42190853506326675\n",
      "epoch 23, batch 20, current loss 0.34445594251155853\n",
      "epoch 23, batch 30, current loss 0.34917358979582785\n",
      "epoch 23, batch 40, current loss 0.2675759755074978\n",
      "epoch 23, batch 50, current loss 0.2501744173467159\n",
      "epoch 24, batch 10, current loss 0.33886082768440245\n",
      "epoch 24, batch 20, current loss 0.43787881433963777\n",
      "epoch 24, batch 30, current loss 0.3413306772708893\n",
      "epoch 24, batch 40, current loss 0.3219473659992218\n",
      "epoch 24, batch 50, current loss 0.40157592445611956\n",
      "epoch 25, batch 10, current loss 0.3333363875746727\n",
      "epoch 25, batch 20, current loss 0.26783288158476354\n",
      "epoch 25, batch 30, current loss 0.4206228509545326\n",
      "epoch 25, batch 40, current loss 0.343700697273016\n",
      "epoch 25, batch 50, current loss 0.4212445065379143\n",
      "epoch 26, batch 10, current loss 0.3807614132761955\n",
      "epoch 26, batch 20, current loss 0.27821540683507917\n",
      "epoch 26, batch 30, current loss 0.33877226151525974\n",
      "epoch 26, batch 40, current loss 0.41554457098245623\n",
      "epoch 26, batch 50, current loss 0.39562193527817724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, batch 10, current loss 0.19959915354847907\n",
      "epoch 27, batch 20, current loss 0.2464704718440771\n",
      "epoch 27, batch 30, current loss 0.23873214796185493\n",
      "epoch 27, batch 40, current loss 0.2725895158946514\n",
      "epoch 27, batch 50, current loss 0.0995297223329544\n",
      "epoch 28, batch 10, current loss 0.2581790404394269\n",
      "epoch 28, batch 20, current loss 0.3013050079345703\n",
      "epoch 28, batch 30, current loss 0.28650456815958025\n",
      "epoch 28, batch 40, current loss 0.3264158096164465\n",
      "epoch 28, batch 50, current loss 0.2546819016337395\n",
      "epoch 29, batch 10, current loss 0.21789034977555274\n",
      "epoch 29, batch 20, current loss 0.15533548034727573\n",
      "epoch 29, batch 30, current loss 0.21806127913296222\n",
      "epoch 29, batch 40, current loss 0.16466109454631805\n",
      "epoch 29, batch 50, current loss 0.17509342525154353\n",
      "epoch 30, batch 10, current loss 0.14866357818245887\n",
      "epoch 30, batch 20, current loss 0.4511845324188471\n",
      "epoch 30, batch 30, current loss 0.22828017249703408\n",
      "epoch 30, batch 40, current loss 0.19435512386262416\n",
      "epoch 30, batch 50, current loss 0.227964548766613\n",
      "epoch 30 train accuracy: \n",
      "total_num:449, test accuracy:0.9844097995545658, positive_acc:0.9917582417582418, negative_acc:0.9529411764705882\n",
      "[[  81.    4.]\n",
      " [   3.  361.]]\n",
      "[129, 207, 315, 374]\n",
      "[49, 169, 435]\n",
      "-------------------\n",
      "epoch 30 test accuracy: \n",
      "total_num:93, test accuracy:0.967741935483871, positive_acc:0.9868421052631579, negative_acc:0.8823529411764706\n",
      "[[ 15.   2.]\n",
      " [  1.  75.]]\n",
      "[78, 92]\n",
      "[23]\n",
      "epoch 31, batch 10, current loss 0.14585557784885167\n",
      "epoch 31, batch 20, current loss 0.22302905302494763\n",
      "epoch 31, batch 30, current loss 0.1093741551041603\n",
      "epoch 31, batch 40, current loss 0.20774608999490737\n",
      "epoch 31, batch 50, current loss 0.32770604230463507\n",
      "epoch 32, batch 10, current loss 0.22180991619825363\n",
      "epoch 32, batch 20, current loss 0.26639218218624594\n",
      "epoch 32, batch 30, current loss 0.22126063331961632\n",
      "epoch 32, batch 40, current loss 0.27514308877289295\n",
      "epoch 32, batch 50, current loss 0.2570696547627449\n",
      "epoch 33, batch 10, current loss 0.23012951463460923\n",
      "epoch 33, batch 20, current loss 0.13254787698388099\n",
      "epoch 33, batch 30, current loss 0.15646571647375823\n",
      "epoch 33, batch 40, current loss 0.17118844529613853\n",
      "epoch 33, batch 50, current loss 0.2274078382179141\n",
      "epoch 34, batch 10, current loss 0.2987159747630358\n",
      "epoch 34, batch 20, current loss 0.23028647918254136\n",
      "epoch 34, batch 30, current loss 0.13273495770990848\n",
      "epoch 34, batch 40, current loss 0.3813157122582197\n",
      "epoch 34, batch 50, current loss 0.21085361763834953\n",
      "epoch 35, batch 10, current loss 0.16507397033274174\n",
      "epoch 35, batch 20, current loss 0.18056699093431233\n",
      "epoch 35, batch 30, current loss 0.18782377280294896\n",
      "epoch 35, batch 40, current loss 0.1267461346462369\n",
      "epoch 35, batch 50, current loss 0.2150844207033515\n",
      "epoch 36, batch 10, current loss 0.13442184431478382\n",
      "epoch 36, batch 20, current loss 0.15042216153815388\n",
      "epoch 36, batch 30, current loss 0.25826671430841086\n",
      "epoch 36, batch 40, current loss 0.145091022644192\n",
      "epoch 36, batch 50, current loss 0.33280471824109553\n",
      "epoch 37, batch 10, current loss 0.21369720362126826\n",
      "epoch 37, batch 20, current loss 0.1171012433245778\n",
      "epoch 37, batch 30, current loss 0.10544136865064502\n",
      "epoch 37, batch 40, current loss 0.1258466082625091\n",
      "epoch 37, batch 50, current loss 0.14688942078500986\n",
      "epoch 38, batch 10, current loss 0.08635053890757263\n",
      "epoch 38, batch 20, current loss 0.1725568759255111\n",
      "epoch 38, batch 30, current loss 0.1716819081455469\n",
      "epoch 38, batch 40, current loss 0.07093474436551332\n",
      "epoch 38, batch 50, current loss 0.08772493600845337\n",
      "epoch 39, batch 10, current loss 0.14766412395983936\n",
      "epoch 39, batch 20, current loss 0.09834698839113117\n",
      "epoch 39, batch 30, current loss 0.32998655503615737\n",
      "epoch 39, batch 40, current loss 0.1556589338928461\n",
      "epoch 39, batch 50, current loss 0.15273902285844088\n",
      "epoch 40, batch 10, current loss 0.09198198523372411\n",
      "epoch 40, batch 20, current loss 0.11322753597050905\n",
      "epoch 40, batch 30, current loss 0.1408783643040806\n",
      "epoch 40, batch 40, current loss 0.12238609697669744\n",
      "epoch 40, batch 50, current loss 0.1835572412237525\n",
      "epoch 40 train accuracy: \n",
      "total_num:449, test accuracy:0.9866369710467706, positive_acc:0.9862637362637363, negative_acc:0.9882352941176471\n",
      "[[  84.    1.]\n",
      " [   5.  359.]]\n",
      "[129]\n",
      "[49, 169, 286, 373, 435]\n",
      "-------------------\n",
      "epoch 40 test accuracy: \n",
      "total_num:93, test accuracy:0.967741935483871, positive_acc:0.9736842105263158, negative_acc:0.9411764705882353\n",
      "[[ 16.   1.]\n",
      " [  2.  74.]]\n",
      "[78]\n",
      "[23, 91]\n",
      "epoch 41, batch 10, current loss 0.15322387898340822\n",
      "epoch 41, batch 20, current loss 0.08536465540528297\n",
      "epoch 41, batch 30, current loss 0.06727244183421136\n",
      "epoch 41, batch 40, current loss 0.1592373587191105\n",
      "epoch 41, batch 50, current loss 0.18136469274759293\n",
      "epoch 42, batch 10, current loss 0.11081699198111891\n",
      "epoch 42, batch 20, current loss 0.10324635757133364\n",
      "epoch 42, batch 30, current loss 0.23097843956202269\n",
      "epoch 42, batch 40, current loss 0.06648811595514417\n",
      "epoch 42, batch 50, current loss 0.14771954203024507\n",
      "epoch 43, batch 10, current loss 0.1491101160645485\n",
      "epoch 43, batch 20, current loss 0.07785549024119973\n",
      "epoch 43, batch 30, current loss 0.1626493876799941\n",
      "epoch 43, batch 40, current loss 0.09356785956770182\n",
      "epoch 43, batch 50, current loss 0.08328468231484294\n",
      "epoch 44, batch 10, current loss 0.05588284539990127\n",
      "epoch 44, batch 20, current loss 0.18351599834859372\n",
      "epoch 44, batch 30, current loss 0.052546075475402174\n",
      "epoch 44, batch 40, current loss 0.2518384350463748\n",
      "epoch 44, batch 50, current loss 0.05951353777199984\n",
      "epoch 45, batch 10, current loss 0.027267865976318716\n",
      "epoch 45, batch 20, current loss 0.14934096108190714\n",
      "epoch 45, batch 30, current loss 0.2232142903842032\n",
      "epoch 45, batch 40, current loss 0.21658008377999066\n",
      "epoch 45, batch 50, current loss 0.08817143966443837\n",
      "epoch 46, batch 10, current loss 0.08070020317099988\n",
      "epoch 46, batch 20, current loss 0.05785766094923019\n",
      "epoch 46, batch 30, current loss 0.1136502874083817\n",
      "epoch 46, batch 40, current loss 0.11300681643188\n",
      "epoch 46, batch 50, current loss 0.07950771856121719\n",
      "epoch 47, batch 10, current loss 0.12681695949286223\n",
      "epoch 47, batch 20, current loss 0.1349863381125033\n",
      "epoch 47, batch 30, current loss 0.11683198483660817\n",
      "epoch 47, batch 40, current loss 0.06274282224476338\n",
      "epoch 47, batch 50, current loss 0.295516510726884\n",
      "epoch 48, batch 10, current loss 0.08743099668063223\n",
      "epoch 48, batch 20, current loss 0.25435086430516096\n",
      "epoch 48, batch 30, current loss 0.0761752687394619\n",
      "epoch 48, batch 40, current loss 0.16093207206577062\n",
      "epoch 48, batch 50, current loss 0.1300183772109449\n",
      "epoch 49, batch 10, current loss 0.06805049167014658\n",
      "epoch 49, batch 20, current loss 0.1560201390646398\n",
      "epoch 49, batch 30, current loss 0.12586245983839034\n",
      "epoch 49, batch 40, current loss 0.051671953801997\n",
      "epoch 49, batch 50, current loss 0.09071231950074435\n",
      "epoch 50, batch 10, current loss 0.02699104119092226\n",
      "epoch 50, batch 20, current loss 0.05169304625596851\n",
      "epoch 50, batch 30, current loss 0.20794993191957473\n",
      "epoch 50, batch 40, current loss 0.05825207174057141\n",
      "epoch 50, batch 50, current loss 0.1476666379487142\n",
      "epoch 50 train accuracy: \n",
      "total_num:449, test accuracy:0.9955456570155902, positive_acc:0.9945054945054945, negative_acc:1.0\n",
      "[[  85.    0.]\n",
      " [   2.  362.]]\n",
      "[]\n",
      "[169, 199]\n",
      "-------------------\n",
      "epoch 50 test accuracy: \n",
      "total_num:93, test accuracy:0.978494623655914, positive_acc:0.9868421052631579, negative_acc:0.9411764705882353\n",
      "[[ 16.   1.]\n",
      " [  1.  75.]]\n",
      "[78]\n",
      "[53]\n",
      "epoch 51, batch 10, current loss 0.06904902299866081\n",
      "epoch 51, batch 20, current loss 0.09631691150134429\n",
      "epoch 51, batch 30, current loss 0.05445891610579565\n",
      "epoch 51, batch 40, current loss 0.05840595993213356\n",
      "epoch 51, batch 50, current loss 0.05484823588631116\n",
      "epoch 52, batch 10, current loss 0.05047309095971286\n",
      "epoch 52, batch 20, current loss 0.06060157318133861\n",
      "epoch 52, batch 30, current loss 0.06028235908597708\n",
      "epoch 52, batch 40, current loss 0.07886117331217975\n",
      "epoch 52, batch 50, current loss 0.05946422587148845\n",
      "epoch 53, batch 10, current loss 0.10897019460098818\n",
      "epoch 53, batch 20, current loss 0.13340257919044235\n",
      "epoch 53, batch 30, current loss 0.056637371936812994\n",
      "epoch 53, batch 40, current loss 0.06847124937921763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53, batch 50, current loss 0.05965827321633697\n",
      "epoch 54, batch 10, current loss 0.12990458267740906\n",
      "epoch 54, batch 20, current loss 0.025350341061130165\n",
      "epoch 54, batch 30, current loss 0.14087775563821198\n",
      "epoch 54, batch 40, current loss 0.20951480297371744\n",
      "epoch 54, batch 50, current loss 0.12560680508613586\n",
      "epoch 55, batch 10, current loss 0.05518680431414395\n",
      "epoch 55, batch 20, current loss 0.12741815439076162\n",
      "epoch 55, batch 30, current loss 0.031238413817482068\n",
      "epoch 55, batch 40, current loss 0.08417800278402865\n",
      "epoch 55, batch 50, current loss 0.08735472063999623\n",
      "epoch 56, batch 10, current loss 0.049083476420491934\n",
      "epoch 56, batch 20, current loss 0.024830990470945835\n",
      "epoch 56, batch 30, current loss 0.03528400203213096\n",
      "epoch 56, batch 40, current loss 0.06776834188494832\n",
      "epoch 56, batch 50, current loss 0.05520205064676702\n",
      "epoch 57, batch 10, current loss 0.05118629848875571\n",
      "epoch 57, batch 20, current loss 0.02148262085975148\n",
      "epoch 57, batch 30, current loss 0.18105564069701358\n",
      "epoch 57, batch 40, current loss 0.11717538489028811\n",
      "epoch 57, batch 50, current loss 0.18083809427917003\n",
      "epoch 58, batch 10, current loss 0.07528757285326719\n",
      "epoch 58, batch 20, current loss 0.06324237915687263\n",
      "epoch 58, batch 30, current loss 0.06938959932886064\n",
      "epoch 58, batch 40, current loss 0.024009315855801107\n",
      "epoch 58, batch 50, current loss 0.07213856803718954\n",
      "epoch 59, batch 10, current loss 0.11626045226003043\n",
      "epoch 59, batch 20, current loss 0.04299472908605821\n",
      "epoch 59, batch 30, current loss 0.12665613064309583\n",
      "epoch 59, batch 40, current loss 0.03495179710444063\n",
      "epoch 59, batch 50, current loss 0.03989653585013002\n",
      "epoch 60, batch 10, current loss 0.12481554348487407\n",
      "epoch 60, batch 20, current loss 0.045256705014617184\n",
      "epoch 60, batch 30, current loss 0.0665879781357944\n",
      "epoch 60, batch 40, current loss 0.07526960403192788\n",
      "epoch 60, batch 50, current loss 0.062154404865577814\n",
      "epoch 60 train accuracy: \n",
      "total_num:449, test accuracy:0.9910913140311804, positive_acc:0.989010989010989, negative_acc:1.0\n",
      "[[  85.    0.]\n",
      " [   4.  360.]]\n",
      "[]\n",
      "[49, 168, 169, 435]\n",
      "-------------------\n",
      "epoch 60 test accuracy: \n",
      "total_num:93, test accuracy:0.946236559139785, positive_acc:0.9473684210526315, negative_acc:0.9411764705882353\n",
      "[[ 16.   1.]\n",
      " [  4.  72.]]\n",
      "[78]\n",
      "[2, 23, 53, 91]\n",
      "epoch 61, batch 10, current loss 0.0381342256674543\n",
      "epoch 61, batch 20, current loss 0.04477425345685333\n",
      "epoch 61, batch 30, current loss 0.01865428570890799\n",
      "epoch 61, batch 40, current loss 0.05657883916283026\n",
      "epoch 61, batch 50, current loss 0.05177101742010563\n",
      "epoch 62, batch 10, current loss 0.046752509829821064\n",
      "epoch 62, batch 20, current loss 0.02327729946700856\n",
      "epoch 62, batch 30, current loss 0.0862744944053702\n",
      "epoch 62, batch 40, current loss 0.16527882788504938\n",
      "epoch 62, batch 50, current loss 0.041512672521639614\n",
      "epoch 63, batch 10, current loss 0.03162859504809603\n",
      "epoch 63, batch 20, current loss 0.0767055265721865\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_net()\n",
    "#net.apply(weight_init)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "print(\"There are {} parameters in the model\".format(count_parameters(net)))\n",
    "\n",
    "num_epochs = 100\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([3.5,1.0]).to(device))\n",
    "\n",
    "optimizer_base = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "optimizer = SWA(optimizer_base, swa_start=60, swa_freq=5)\n",
    "\n",
    "print('choose SGD as optimizer')\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr*10, weight_decay=0.00001)\n",
    "#print('choose Adam as optimizer')\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=70, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    \n",
    "    Mouse_dataset = Mouse_sub_volumes(train_data)#transform=transforms.Compose([Flip()])\n",
    "    dataloader = DataLoader(Mouse_dataset, batch_size=8, shuffle=True, num_workers=4, drop_last = True)\n",
    "    train(net, device, dataloader, optimizer, criterion, epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch {} train accuracy: '.format(epoch+1))\n",
    "        train_Mouse_dataset = Mouse_sub_volumes(train_data)\n",
    "        train_dataloader = DataLoader(train_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        train_dic = test(net, device, train_dataloader)\n",
    "        get_confusion_matrix(train_dic)\n",
    "        \n",
    "        print(\"-------------------\")\n",
    "        print('epoch {} test accuracy: '.format(epoch+1))\n",
    "        test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "        test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        test_dic = test(net, device, test_dataloader)\n",
    "        get_confusion_matrix(test_dic)\n",
    "        \n",
    "        torch.save(net.state_dict(), './model/mut_clas_2020_02_18_e{}_bv_body.pth'.format(epoch+1))\n",
    "        \n",
    "train_test_data = [train_data, test_data]\n",
    "save_file = open('./model/mut_clas_2020_02_18_e{}_bv_body_data.pickle'.format(epoch+1),'wb')\n",
    "pickle.dump(train_test_data, save_file)\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_net()\n",
    "#net.apply(weight_init)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "print(\"There are {} parameters in the model\".format(count_parameters(net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_probability(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    \n",
    "    true_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "            inputs = inputs.to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            outputs = F.softmax(outputs)\n",
    "            true_predicted_labels.append((labels.numpy(), predicted.cpu().numpy(), outputs.cpu().numpy()[0,0], outputs.cpu().numpy()[0,1]))\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    return true_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), './model/mut_clas_2019_01_15.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./model/mut_clas_2020_02_14_e120_bv_body.pth'))\n",
    "\n",
    "with open('./model/mut_clas_2020_02_14_e120_bv_body_data.pickle', 'rb') as input_file:\n",
    "    train_data, test_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train accuracy: ')\n",
    "Mouse_dataset = Mouse_sub_volumes(train_data)\n",
    "train_dataloader = DataLoader(Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "train_dic = test_with_probability(net, device, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy: ')\n",
    "Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "test_dataloader = DataLoader(Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dic = test_with_probability(net, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_table = np.zeros([2,2])\n",
    "mut_to_nor = []\n",
    "nor_to_mul = []\n",
    "\n",
    "for i in range(len(test_dic)):\n",
    "    if test_dic[i][0] ==0 and test_dic[i][1] ==0:\n",
    "        cross_table[0,0] += 1\n",
    "    elif  test_dic[i][0] ==0 and test_dic[i][1] ==1:\n",
    "        cross_table[0,1] += 1\n",
    "        mut_to_nor.append(i)\n",
    "    elif test_dic[i][0] ==1 and test_dic[i][1] ==0:\n",
    "        cross_table[1,0] += 1\n",
    "        nor_to_mul.append(i)\n",
    "    elif test_dic[i][0] ==1 and test_dic[i][1] ==1:\n",
    "        cross_table[1,1] += 1\n",
    "print(cross_table)\n",
    "print(mut_to_nor)\n",
    "print(nor_to_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    ##############################################################################\n",
    "    # Perform a forward and backward pass through the model to compute the gradient \n",
    "    # of the correct class score with respect to each input image. You first want \n",
    "    # to compute the loss over the correct scores (we'll combine losses across a batch\n",
    "    # by summing), and then compute the gradients with a backward pass.\n",
    "    ##############################################################################\n",
    "    scores = model(X)\n",
    "    \n",
    "    # Get the correct class computed scores.\n",
    "    scores = scores.gather(1, y.view(-1, 1)).squeeze()  \n",
    "    \n",
    "    # Backward pass, need to supply initial gradients of same tensor shape as scores.\n",
    "    scores.backward(torch.tensor(10.0).cuda(device))\n",
    "    \n",
    "    # Get gradient for image.\n",
    "    saliency = X.grad.data\n",
    "    \n",
    "    # Convert from 3d to 1d.\n",
    "    saliency = saliency.abs()\n",
    "    saliency = saliency.squeeze()\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Mouse_dataset = Mouse_sub_volumes(test_data)\n",
    "test_dataloader = DataLoader(test_Mouse_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "    inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    saliency = compute_saliency_maps(inputs, labels, net)\n",
    "    \n",
    "    max_value = torch.max(saliency)\n",
    "    print(torch.max(saliency), torch.min(saliency))\n",
    "    saliency[saliency >= (max_value*0.2)] = 100\n",
    "    saliency[saliency < (max_value*0.2)] = 0\n",
    "    saliency *= 0.01\n",
    "    \n",
    "    img_nft = nib.Nifti1Image(np.squeeze(inputs.cpu().detach().numpy()+1.0),np.eye(4))\n",
    "    img_save_data_path = './saliency_map/img_label{}_{}.nii'.format(labels.cpu().numpy()[0], i_batch)\n",
    "    nib.save(img_nft,img_save_data_path)\n",
    "    \n",
    "    print(i_batch, ' saliency num: ', np.sum(saliency.cpu().numpy()))\n",
    "    saliency_nft = nib.Nifti1Image(np.squeeze(saliency.cpu().numpy()),np.eye(4))\n",
    "    saliency_save_data_path = './saliency_map/salency_label{}_{}.nii'.format(labels.cpu().numpy()[0], i_batch)\n",
    "    nib.save(saliency_nft,saliency_save_data_path)\n",
    "    \n",
    "# 5,6,8,9,10,11,12,33,27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
