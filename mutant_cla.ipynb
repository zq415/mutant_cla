{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def read_mutant_txt(path):\n",
    "    name_list = []\n",
    "    fo = open(path)\n",
    "    for line in fo:\n",
    "        striped_line = line.strip('\\n')\n",
    "        if striped_line != '':\n",
    "            name_list.append(striped_line)\n",
    "    return name_list\n",
    "\n",
    "def find_centroid(bv_img):\n",
    "    bv_voxel_num = np.sum(bv_img)\n",
    "    # find x_centroid\n",
    "    x_centroid = 0\n",
    "    for i in range(bv_img.shape[0]):\n",
    "        x_centroid += np.sum(bv_img[i,:,:])*i\n",
    "    x_centroid /= bv_voxel_num\n",
    "    # find y_centroid\n",
    "    y_centroid = 0\n",
    "    for i in range(bv_img.shape[1]):\n",
    "        y_centroid += np.sum(bv_img[:,i,:])*i\n",
    "    y_centroid /= bv_voxel_num\n",
    "    # find z_centroid\n",
    "    z_centroid = 0\n",
    "    for i in range(bv_img.shape[2]):\n",
    "        z_centroid += np.sum(bv_img[:,:,i])*i\n",
    "    z_centroid /= bv_voxel_num\n",
    "    return (round(x_centroid), round(y_centroid), round(z_centroid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_func(x):\n",
    "    if x % 2 == 0:\n",
    "        return x\n",
    "    else:\n",
    "        return x+1\n",
    "    \n",
    "def padded_minimum_size(label, min_size):\n",
    "    img_size= np.shape(label)\n",
    "    x_offset=0\n",
    "    y_offset=0\n",
    "    z_offset=0\n",
    "    x_flag=False\n",
    "    y_flag=False\n",
    "    z_flag=False\n",
    "    if img_size[0]<min_size:\n",
    "        x_offset=min_size-img_size[0]\n",
    "        x_flag=True\n",
    "    if img_size[1]<min_size:\n",
    "        y_offset=min_size-img_size[1]\n",
    "        y_flag=True\n",
    "    if img_size[2]<min_size:\n",
    "        z_offset=min_size-img_size[2]\n",
    "        z_flag=True\n",
    "    \n",
    "    if x_flag == True and y_flag == True and z_flag == True:\n",
    "        padded_label=np.zeros((min_size,min_size,min_size),np.uint8)\n",
    "        \n",
    "        padded_label[round(x_offset/2):round(x_offset/2)+img_size[0], \n",
    "                     round(y_offset/2):round(y_offset/2)+img_size[1], \n",
    "                     round(z_offset/2):round(z_offset/2)+img_size[2]]=label\n",
    "        \n",
    "        return padded_label\n",
    "    \n",
    "    elif x_flag == True and y_flag == True:\n",
    "        padded_label=np.zeros((min_size,min_size,even_func(img_size[2])),np.uint8)\n",
    "        \n",
    "        padded_label[round(x_offset/2):round(x_offset/2)+img_size[0],\n",
    "                     round(y_offset/2):round(y_offset/2)+img_size[1],\n",
    "                     0:img_size[2]] = label\n",
    "        return padded_label\n",
    "    \n",
    "    elif x_flag == True and z_flag == True:\n",
    "        padded_label=np.zeros((min_size,even_func(img_size[1]),min_size),np.uint8)\n",
    "        \n",
    "        padded_label[round(x_offset/2):round(x_offset/2)+img_size[0],\n",
    "                     0:img_size[1],\n",
    "                     round(z_offset/2):round(z_offset/2)+img_size[2]]=label\n",
    "        return padded_label\n",
    "    \n",
    "    elif y_flag == True and z_flag == True:\n",
    "        padded_label=np.zeros((even_func(img_size[0]),min_size,min_size),np.uint8)\n",
    "        \n",
    "        padded_label[0:img_size[0],\n",
    "                     round(y_offset/2):round(y_offset/2)+img_size[1],\n",
    "                     round(z_offset/2):round(z_offset/2)+img_size[2]]=label\n",
    "        return padded_label\n",
    "    \n",
    "    elif x_flag == True:\n",
    "        padded_label=np.zeros((min_size,even_fuc(img_size[1]),even_func(img_size[2])),np.uint8)\n",
    "      \n",
    "        padded_label[round(x_offset/2):round(x_offset/2)+img_size[0],\n",
    "                     0:img_size[1],\n",
    "                     0:img_size[2]]=label\n",
    "        return padded_label\n",
    "    \n",
    "    elif y_flag == True:\n",
    "        padded_label=np.zeros((even_func(img_size[0]),min_size,even_func(img_size[2])),np.uint8)\n",
    "       \n",
    "        padded_label[0:img_size[0],\n",
    "                     round(y_offset/2):round(y_offset/2)+img_size[1],\n",
    "                     0:img_size[2]]=label\n",
    "        return padded_label\n",
    "    \n",
    "    elif z_flag == True:\n",
    "        padded_label=np.zeros((even_func(img_size[0]),even_func(img_size[1]),min_size),np.uint8)\n",
    "\n",
    "        padded_label[0:img_size[0],\n",
    "                     0:img_size[1],\n",
    "                     round(z_offset/2):round(z_offset/2)+img_size[2]]=label\n",
    "        return padded_label\n",
    "    \n",
    "    else:\n",
    "        padded_label=np.zeros((even_func(img_size[0]),even_func(img_size[1]),even_func(img_size[2])),np.uint8)\n",
    "       \n",
    "        padded_label[0:img_size[0],\n",
    "                     0:img_size[1],\n",
    "                     0:img_size[2]]=label\n",
    "        return padded_label\n",
    "\n",
    "def extract_positive(whole_label,box_size,smallest_ratio,step_size,mul_label,img_idx):\n",
    "    img_size=np.shape(whole_label)\n",
    "    bv_voxel_num=np.sum(whole_label)\n",
    "    positive_sub_volumes=[]\n",
    "    x_slice,y_slice,z_slice = ndimage.find_objects(whole_label)[0]\n",
    "    offset=5\n",
    "    \n",
    "    x_start = x_slice.stop-offset if x_slice.stop-offset >= box_size else box_size\n",
    "    x_stop = x_slice.start+box_size+offset if x_slice.start+box_size+offset <= img_size[0] else img_size[0]\n",
    "    \n",
    "    y_start = y_slice.stop-offset if y_slice.stop-offset >= box_size else box_size\n",
    "    y_stop = y_slice.start+box_size+offset if y_slice.start+box_size+offset <= img_size[1] else img_size[1]\n",
    "    \n",
    "    z_start = z_slice.stop-offset if z_slice.stop-offset >= box_size else box_size\n",
    "    z_stop = z_slice.start+box_size+offset if z_slice.start+box_size+offset <= img_size[2] else img_size[2]\n",
    "    \n",
    "    for i in range(x_start,x_stop+1,step_size):\n",
    "        for j in range(y_start,y_stop+1,step_size):\n",
    "            for k in range(z_start,z_stop+1,step_size):\n",
    "                contain_ratio = np.sum(whole_label[i-box_size:i,\n",
    "                                                   j-box_size:j,\n",
    "                                                   k-box_size:k])/(bv_voxel_num+0.001)\n",
    "            \n",
    "                if contain_ratio > smallest_ratio:\n",
    "                    positive_sub_volumes.append((img_idx,mul_label,(i,j,k)))\n",
    "    return positive_sub_volumes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutant_names = read_mutant_txt('mutant_imgs.txt')\n",
    "data_base_path = '/scratch/zq415/grammar_cor/Localization/data'\n",
    "data_folder_list = ['20180419_newdata_nii_with_filtered', 'new_data_20180522_nii', 'organized_data_nii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n"
     ]
    }
   ],
   "source": [
    "all_BVs = []\n",
    "for cur_floder in data_folder_list:\n",
    "    cur_folder_path = os.path.join(data_base_path,cur_floder)\n",
    "    all_BVs += glob.glob(cur_folder_path+'/*/*/*[Bb][Vv]*')\n",
    "print(len(all_BVs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dic = {}\n",
    "same_name_num = 0\n",
    "for full_bv_path in all_BVs:\n",
    "    bv_file_name = os.path.basename(full_bv_path)\n",
    "    if 'BV' in bv_file_name:\n",
    "        if bv_file_name[:-14] in all_data_dic:\n",
    "            same_name_num += 1\n",
    "            continue\n",
    "        all_data_dic[bv_file_name[:-14]] = full_bv_path\n",
    "    else:\n",
    "        if bv_file_name[:-9] in all_data_dic:\n",
    "            same_name_num += 1\n",
    "            continue\n",
    "        all_data_dic[bv_file_name[:-9]] = full_bv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data_dic = {}\n",
    "# same_name_num = 0\n",
    "# for full_bv_path in all_BVs:\n",
    "#     bv_file_name = os.path.basename(full_bv_path)\n",
    "#     if 'BV' in bv_file_name:\n",
    "#         if bv_file_name[:-14] in all_data_dic:\n",
    "#             same_name_num += 1\n",
    "#             continue\n",
    "#         all_data_dic[bv_file_name[:-14]] = (full_bv_path[:-14] + '.nii', full_bv_path)\n",
    "#     else:\n",
    "#         if bv_file_name[:-9] in all_data_dic:\n",
    "#             same_name_num += 1\n",
    "#             continue\n",
    "#         all_data_dic[bv_file_name[:-9]] = (full_bv_path[:-9] + '_2.nii', full_bv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 0 \n",
    "\n",
    "# for key in all_data_dic:\n",
    "#     #print(all_data_dic[key])\n",
    "# #     print(key)\n",
    "#     if 'new_data_20180522_nii' in all_data_dic[key]:\n",
    "#         print(num)\n",
    "#     num += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import copyfile\n",
    "# dst = '/scratch/zq415/grammar_cor/mutant_detect/data/mutant_img/'\n",
    "\n",
    "# all_train_data = []\n",
    "# all_idx = []\n",
    "# img_num = 0\n",
    "# normal_num = 0\n",
    "# mutant_num = 0\n",
    "# for key in all_data_dic:\n",
    "#     if key in mutant_names:\n",
    "#         bv_base_name = os.path.basename(all_data_dic[key][1])\n",
    "#         copyfile(all_data_dic[key][1], dst + bv_base_name)\n",
    "        \n",
    "#         img_base_name = os.path.basename(all_data_dic[key][0])\n",
    "#         copyfile(all_data_dic[key][0], dst + img_base_name)\n",
    "        \n",
    "# #         bv_label = nib.load(all_data_dic[key][1])\n",
    "# #         bv_label = bv_label.get_data()\n",
    "# #         #bv_label = padded_minimum_size(bv_label.get_data(),128)\n",
    "\n",
    "# #         img = nib.load(all_data_dic[key][0])\n",
    "# #         img = img.get_data()\n",
    "# #         #img = padded_minimum_size(img.get_data(),128)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_data = []\n",
    "# all_idx = []\n",
    "# img_num = 0\n",
    "# normal_num = 0\n",
    "# mutant_num = 0\n",
    "# for key in all_data_dic:\n",
    "#     bv_label = nib.load(all_data_dic[key])\n",
    "#     bv_label = padded_minimum_size(bv_label.get_data(),128)\n",
    "#     bv_label=np.round(zoom(bv_label,1/2))\n",
    "#     #bv_centroid = find_centroid(bv_label)\n",
    "#     all_train_data.append(bv_label)\n",
    "#     if key in mutant_names:\n",
    "# #def extract_positive(whole_label,box_size,smallest_ratio,step_size,mul_label,img_idx):\n",
    "#         all_idx += extract_positive(bv_label,64,0.94,2,0,img_num)\n",
    "#         mutant_num += 1\n",
    "#         img_num += 1\n",
    "#     else:\n",
    "#         all_idx += extract_positive(bv_label,64,0.99,3,1,img_num)\n",
    "#         normal_num += 1\n",
    "#         img_num += 1\n",
    "#     print('train sample number: ', img_num)\n",
    "#     assert(len(all_train_data)==img_num)\n",
    "# print('mutant number: ', mutant_num)\n",
    "# print('normal number: ', normal_num)\n",
    "# print('train sample number: ', len(all_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = [all_train_data, all_idx]\n",
    "#save_name = 'All_train_bv_data.pickle'\n",
    "#save_file = open(os.path.join(os.getcwd(),'data',save_name),'wb')\n",
    "#pickle.dump(all_data,save_file)\n",
    "#save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'All_train_bv_data.pickle'\n",
    "with open(os.path.join(os.getcwd(),'data',save_name), \"rb\") as input_file:\n",
    "    all_train_data, all_idx = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147983 131853 279836\n"
     ]
    }
   ],
   "source": [
    "mut_sample_num = 0\n",
    "normal_sample_num = 0\n",
    "for i in range(len(all_idx)):\n",
    "    if all_idx[i][1] == 0:\n",
    "        mut_sample_num += 1\n",
    "    if all_idx[i][1] == 1:\n",
    "        normal_sample_num += 1\n",
    "print(mut_sample_num, normal_sample_num, len(all_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 [1, 3, 25, 48, 49, 51, 84, 91, 92, 93, 95, 103, 105, 124, 135, 139, 152, 153, 186, 187, 188, 189, 190, 192, 193, 194, 206, 216, 218, 221, 234, 235, 237, 239, 253, 255, 256, 257, 258, 268, 290, 293, 294, 300, 304, 305, 306, 307, 312, 316, 333, 334, 336, 340, 341, 352, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 405, 407, 413, 416, 417, 419, 420, 421, 427, 428, 429, 434, 438, 440, 451, 452, 453, 454, 455, 458, 459, 460, 484, 496, 497, 499, 507, 508, 509, 520, 521, 523, 546, 548, 553, 555, 557]\n",
      "462 [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 94, 96, 97, 98, 99, 100, 101, 102, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 191, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 236, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 295, 296, 297, 298, 299, 301, 302, 303, 308, 309, 310, 311, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 335, 337, 338, 339, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 364, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 406, 408, 409, 410, 411, 412, 414, 415, 418, 422, 423, 424, 425, 426, 430, 431, 432, 433, 435, 436, 437, 439, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 456, 457, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 498, 500, 501, 502, 503, 504, 505, 506, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 547, 549, 550, 551, 552, 554, 556, 558, 559, 560, 561, 562, 563, 564]\n"
     ]
    }
   ],
   "source": [
    "mutant_list = []\n",
    "normal_list = []\n",
    "img_num = 0\n",
    "for i in range(len(all_idx)):\n",
    "    if all_idx[i][1] == 0:\n",
    "        mutant_list.append(all_idx[i][0])\n",
    "    else:\n",
    "        normal_list.append(all_idx[i][0])\n",
    "print(len(set(mutant_list)), list(np.sort(list(set(mutant_list)))))\n",
    "print(len(set(normal_list)), list(np.sort(list(set(normal_list)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutant:  [1, 3, 25, 48, 49, 51, 84, 91, 92, 93, 95, 103, 105, 124, 135, 139, 152, 153, 186, 187, 188, 189, 190, 192, 193, 194, 206, 216, 218, 221, 234, 235, 237, 239, 253, 255, 256, 257, 258, 268, 290, 293, 294, 300, 304, 305, 306, 307, 312, 316, 333, 334, 336, 340, 341, 352, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 405, 407, 413, 416, 417, 419, 420, 421, 427, 428, 429, 434, 438, 440, 451, 452, 453, 454, 455, 458, 459, 460, 484, 496, 497, 499, 507, 508, 509, 520, 521, 523, 546, 548, 553, 555, 557]\n",
      "normal:  [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 94, 96, 97, 98, 99, 100, 101, 102, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 191, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 236, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 295, 296, 297, 298, 299, 301, 302, 303, 308, 309, 310, 311, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 335, 337, 338, 339, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 364, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 406, 408, 409, 410, 411, 412, 414, 415, 418, 422, 423, 424, 425, 426, 430, 431, 432, 433, 435, 436, 437, 439, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 456, 457, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 498, 500, 501, 502, 503, 504, 505, 506, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 547, 549, 550, 551, 552, 554, 556, 558, 559, 560, 561, 562, 563, 564]\n"
     ]
    }
   ],
   "source": [
    "mutant_list = list(np.sort(list(set(mutant_list))))\n",
    "normal_list = list(np.sort(list(set(normal_list))))\n",
    "print('mutant: ', mutant_list)\n",
    "print('normal: ', normal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_nft = nib.Nifti1Image(np.squeeze(all_train_data[193]),np.eye(4))\n",
    "#img_save_data_path = './img/mul_img.nii'\n",
    "#nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(test_mutant_list))\n",
    "print(len(test_normal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mutant_list [255, 256, 257, 258, 268, 290, 293, 294, 300, 304, 305, 306, 307, 312, 316]\n",
      "train_mutant_list [1, 3, 25, 48, 49, 51, 84, 91, 92, 93, 95, 103, 105, 124, 135, 139, 152, 153, 186, 187, 188, 189, 190, 192, 193, 194, 206, 216, 218, 221, 234, 235, 237, 239, 253, 333, 334, 336, 340, 341, 352, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 405, 407, 413, 416, 417, 419, 420, 421, 427, 428, 429, 434, 438, 440, 451, 452, 453, 454, 455, 458, 459, 460, 484, 496, 497, 499, 507, 508, 509, 520, 521, 523, 546, 548, 553, 555, 557]\n",
      "test_normal_list [259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 295, 296, 297, 298, 299, 301, 302, 303, 308, 309, 310, 311, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 335, 337, 338, 339, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 364, 367, 368, 369]\n",
      "train_normal_list [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 94, 96, 97, 98, 99, 100, 101, 102, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 191, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 236, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 406, 408, 409, 410, 411, 412, 414, 415, 418, 422, 423, 424, 425, 426, 430, 431, 432, 433, 435, 436, 437, 439, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 456, 457, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 498, 500, 501, 502, 503, 504, 505, 506, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 547, 549, 550, 551, 552, 554, 556, 558, 559, 560, 561, 562, 563, 564]\n"
     ]
    }
   ],
   "source": [
    "test_mutant_list = [i for i in mutant_list if i in range(255,320)]\n",
    "test_normal_list = [i for i in normal_list if i in range(255,370)]\n",
    "train_mutant_list = [x for x in mutant_list if x not in test_mutant_list]\n",
    "train_normal_list = [x for x in normal_list if x not in test_normal_list]\n",
    "print('test_mutant_list', test_mutant_list)\n",
    "print('train_mutant_list', train_mutant_list)\n",
    "print('test_normal_list', test_normal_list)\n",
    "print('train_normal_list', train_normal_list)\n",
    "test_img_list = test_mutant_list + test_normal_list\n",
    "train_img_list = train_mutant_list + train_normal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mutant_list = mutant_list[2:len(mutant_list):9]\n",
    "# test_normal_list = normal_list[2:len(normal_list):9]\n",
    "# train_mutant_list = [x for x in mutant_list if x not in test_mutant_list]\n",
    "# train_normal_list = [x for x in normal_list if x not in test_normal_list]\n",
    "# print('test_mutant_list', test_mutant_list)\n",
    "# print('train_mutant_list', train_mutant_list)\n",
    "# print('test_normal_list', test_normal_list)\n",
    "# print('train_normal_list', train_normal_list)\n",
    "# test_img_list = test_mutant_list + test_normal_list\n",
    "# train_img_list = train_mutant_list + train_normal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = []\n",
    "train_idx = []\n",
    "for i in range(len(all_idx)):\n",
    "    if all_idx[i][0] in test_img_list:\n",
    "        test_idx.append(all_idx[i])\n",
    "    elif all_idx[i][0] in train_img_list:\n",
    "        train_idx.append(all_idx[i])\n",
    "    else:\n",
    "        print('something wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Mouse_sub_volumes(Dataset):\n",
    "    \"\"\"Mouse sub-volumes BV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, all_whole_volumes, all_idx, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            all_whole_volumes: Contain all the padded whole BV volumes as a dic\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.whole_volumes = all_whole_volumes\n",
    "        self.idx = all_idx\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, num):\n",
    "        #idx [0] dictionary index, [1] label(0 or 1), [2] x, y, z sub-volumes index\n",
    "        box_size=64\n",
    "        current_img = self.idx[num][0]\n",
    "        label = self.idx[num][1]\n",
    "        x, y, z = self.idx[num][2]\n",
    "        img = self.whole_volumes[current_img][x-box_size:x,\n",
    "                                             y-box_size:y,\n",
    "                                             z-box_size:z]\n",
    "        img = np.float32(img[np.newaxis,...])\n",
    "        sample = {'image': img, 'label': label, 'img_idx': current_img}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotate(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Rotate the image for data augmentation, but prefer original image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,ori_probability=0.30):\n",
    "        self.ori_probability = ori_probability\n",
    "        #1:(0,0,0), 2:(90,0,0), 3:(180,0,0), 4:(270,0,0), 5:(0,90,0), 6:(0,270,0)\n",
    "        self.face_to_you = [1,2,3,4,5,6]\n",
    "        #rotate along z axis, so that we 24 combination totally\n",
    "        # 1:0 degree, 2: 90 degree, 3: 180 degree, 4: 270 degree\n",
    "        self.rotate_z = [1,2,3,4]\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0,1) < self.ori_probability:\n",
    "            return sample\n",
    "        else:\n",
    "            img, label, current_img = sample['image'], sample['label'], sample['img_idx']\n",
    "            random_choise1=random.choice(self.face_to_you)\n",
    "            rotated_img1 = {1: lambda x: x,\n",
    "                            2: lambda x: ndimage.rotate(x,90,(1,2),reshape='True',mode = 'nearest'),\n",
    "                            3: lambda x: ndimage.rotate(x,180,(1,2),reshape='True',mode = 'nearest'),\n",
    "                            4: lambda x: ndimage.rotate(x,270,(1,2),reshape='True',mode = 'nearest'),\n",
    "                            5: lambda x: ndimage.rotate(x,90,(0,2),reshape='True',mode = 'nearest'),\n",
    "                            6: lambda x: ndimage.rotate(x,270,(0,2),reshape='True',mode = 'nearest')\n",
    "                            }[random_choise1](img[0,...])\n",
    "            random_choise2=random.choice(self.rotate_z)\n",
    "            img[0,...] = {1: lambda x: x,\n",
    "                            2: lambda x: ndimage.rotate(x,90,(0,1),reshape='True',mode = 'nearest'),\n",
    "                            3: lambda x: ndimage.rotate(x,180,(0,1),reshape='True',mode = 'nearest'),\n",
    "                            4: lambda x: ndimage.rotate(x,270,(0,1),reshape='True',mode = 'nearest')\n",
    "                            }[random_choise2](rotated_img1)\n",
    "                \n",
    "        return {'image': img, 'label': label, 'img_idx': current_img}\n",
    "\n",
    "class Flip(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Flip the image for data augmentation, but prefer original image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,ori_probability=0.30):\n",
    "        self.ori_probability = ori_probability\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0,1) < self.ori_probability:\n",
    "            return sample\n",
    "        else:\n",
    "            img, label, current_img = sample['image'], sample['label'], sample['img_idx']\n",
    "            random_choise1=random.choice([1,2,3,4,5,6,7,8])\n",
    "            img[0,...] = {1: lambda x: x,\n",
    "                          2: lambda x: x[::-1,:,:],\n",
    "                          3: lambda x: x[:,::-1,:],\n",
    "                          4: lambda x: x[:,:,::-1],\n",
    "                          5: lambda x: x[::-1,::-1,:],\n",
    "                          6: lambda x: x[::-1,:,::-1],\n",
    "                          7: lambda x: x[:,::-1,::-1],\n",
    "                          8: lambda x: x[::-1,::-1,::-1]\n",
    "                          }[random_choise1](img[0,...])\n",
    "        return {'image': img, 'label': label, 'img_idx': current_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class VGG_net(nn.Module):\n",
    "    def __init__(self,conv_drop_rate=0.15,linear_drop_rate=0.4):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm3d(8)\n",
    "        self.conv2 = nn.Conv3d(in_channels=8, out_channels=8, kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm3d(8)\n",
    "        self.pool1 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout1 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(in_channels=8, out_channels=12, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm3d(12)\n",
    "        self.conv4 = nn.Conv3d(in_channels=12, out_channels=12, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm3d(12)\n",
    "        self.pool2 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout2 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv5 = nn.Conv3d(in_channels=12, out_channels=18, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm3d(18)\n",
    "        self.conv6 = nn.Conv3d(in_channels=18, out_channels=18, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv6_bn = nn.BatchNorm3d(18)\n",
    "        self.pool3 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout3 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.conv7 = nn.Conv3d(in_channels=18, out_channels=24, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv7_bn = nn.BatchNorm3d(24)\n",
    "        self.conv8 = nn.Conv3d(in_channels=24, out_channels=24, kernel_size=3,stride=1, padding=1)\n",
    "        self.conv8_bn = nn.BatchNorm3d(24)\n",
    "        self.pool4 = nn.MaxPool3d(2, 2)\n",
    "        self.dropout4 = nn.Dropout3d(conv_drop_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*4*24, 128)\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.dropout5 = nn.Dropout(linear_drop_rate)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.conv1_bn(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(self.pool1(self.conv2_bn(F.relu(self.conv2(x)))))\n",
    "        \n",
    "        x = self.conv3_bn(F.relu(self.conv3(x)))\n",
    "        x = self.dropout2(self.pool2(self.conv4_bn(F.relu(self.conv4(x)))))\n",
    "        \n",
    "        x = self.conv5_bn(F.relu(self.conv5(x)))\n",
    "        x = self.dropout3(self.pool3(self.conv6_bn(F.relu(self.conv6(x)))))\n",
    "        \n",
    "        x = self.conv7_bn(F.relu(self.conv7(x)))\n",
    "        x = self.dropout4(self.pool4(self.conv8_bn(F.relu(self.conv8(x)))))\n",
    "        \n",
    "        x = x.view(-1, 4*4*4*24)\n",
    "        x = self.dropout5(self.fc1_bn(F.relu(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i_batch % 10 == 0:\n",
    "            print(\"epoch {}, batch {}, current loss {}\".format(epoch+1,i_batch,running_loss/10))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n",
      "There are 247842 parameters in the model\n",
      "choose SGD as optimizer\n",
      "epoch 1, batch 0, current loss 0.07530946135520936\n",
      "epoch 1, batch 10, current loss 0.7062430560588837\n",
      "epoch 1, batch 20, current loss 0.652978903055191\n",
      "epoch 1, batch 30, current loss 0.627645218372345\n",
      "epoch 1, batch 40, current loss 0.5988895893096924\n",
      "epoch 1, batch 50, current loss 0.5971351623535156\n",
      "epoch 1, batch 60, current loss 0.5916464686393738\n",
      "epoch 1, batch 70, current loss 0.5681986391544342\n",
      "epoch 1, batch 80, current loss 0.5575403988361358\n",
      "epoch 1, batch 90, current loss 0.5426340520381927\n",
      "epoch 1, batch 100, current loss 0.5271922469139099\n",
      "epoch 1, batch 110, current loss 0.5266867458820343\n",
      "epoch 1, batch 120, current loss 0.5251856058835983\n",
      "epoch 1, batch 130, current loss 0.4791441559791565\n",
      "epoch 1, batch 140, current loss 0.4685778945684433\n",
      "epoch 1, batch 150, current loss 0.49620257019996644\n",
      "epoch 1, batch 160, current loss 0.48142257928848264\n",
      "epoch 1, batch 170, current loss 0.4554237276315689\n",
      "epoch 1, batch 180, current loss 0.40675453543663026\n",
      "epoch 1, batch 190, current loss 0.39722326397895813\n",
      "epoch 1, batch 200, current loss 0.38589979112148287\n",
      "epoch 1, batch 210, current loss 0.3854328542947769\n",
      "epoch 1, batch 220, current loss 0.37276423871517184\n",
      "epoch 1, batch 230, current loss 0.3450497269630432\n",
      "epoch 1, batch 240, current loss 0.35201727151870726\n",
      "epoch 1, batch 250, current loss 0.31104375422000885\n",
      "epoch 1, batch 260, current loss 0.29505801796913145\n",
      "epoch 1, batch 270, current loss 0.30400319397449493\n",
      "epoch 1, batch 280, current loss 0.24871878176927567\n",
      "epoch 1, batch 290, current loss 0.25789566785097123\n",
      "epoch 1, batch 300, current loss 0.2680582210421562\n",
      "epoch 1, batch 310, current loss 0.19923320710659026\n",
      "epoch 1, batch 320, current loss 0.24001528918743134\n",
      "epoch 1, batch 330, current loss 0.2269417867064476\n",
      "epoch 1, batch 340, current loss 0.19844943881034852\n",
      "epoch 1, batch 350, current loss 0.18800066709518432\n",
      "epoch 1, batch 360, current loss 0.1697766363620758\n",
      "epoch 1, batch 370, current loss 0.14611926376819612\n",
      "epoch 1, batch 380, current loss 0.17158174142241478\n",
      "epoch 1, batch 390, current loss 0.1718934915959835\n",
      "epoch 1, batch 400, current loss 0.167434448748827\n",
      "epoch 1, batch 410, current loss 0.16388460993766785\n",
      "epoch 1, batch 420, current loss 0.11252086162567139\n",
      "epoch 1, batch 430, current loss 0.12234451472759247\n",
      "epoch 1, batch 440, current loss 0.12392338365316391\n",
      "epoch 1, batch 450, current loss 0.10240868292748928\n",
      "epoch 1, batch 460, current loss 0.10043601021170616\n",
      "epoch 1, batch 470, current loss 0.13026733696460724\n",
      "epoch 1, batch 480, current loss 0.10076757185161114\n",
      "epoch 1, batch 490, current loss 0.10741165578365326\n",
      "epoch 1, batch 500, current loss 0.13147087469697\n",
      "epoch 1, batch 510, current loss 0.1151497032493353\n",
      "epoch 1, batch 520, current loss 0.09823170006275177\n",
      "epoch 1, batch 530, current loss 0.10482938699424267\n",
      "epoch 1, batch 540, current loss 0.09378611519932747\n",
      "epoch 1, batch 550, current loss 0.10494883358478546\n",
      "epoch 1, batch 560, current loss 0.08760924935340882\n",
      "epoch 1, batch 570, current loss 0.08746111690998078\n",
      "epoch 1, batch 580, current loss 0.10070463083684444\n",
      "epoch 1, batch 590, current loss 0.08493095487356186\n",
      "epoch 1, batch 600, current loss 0.08881880715489388\n",
      "epoch 1, batch 610, current loss 0.09042283967137336\n",
      "epoch 1, batch 620, current loss 0.07599112838506698\n",
      "epoch 1, batch 630, current loss 0.07529301382601261\n",
      "epoch 1, batch 640, current loss 0.061647039465606214\n",
      "epoch 1, batch 650, current loss 0.07744079641997814\n",
      "epoch 1, batch 660, current loss 0.06214282773435116\n",
      "epoch 1, batch 670, current loss 0.05328326113522053\n",
      "epoch 1, batch 680, current loss 0.05130872093141079\n",
      "epoch 1, batch 690, current loss 0.04451654311269522\n",
      "epoch 2, batch 0, current loss 0.002906036376953125\n",
      "epoch 2, batch 10, current loss 0.07230888232588768\n",
      "epoch 2, batch 20, current loss 0.05646136626601219\n",
      "epoch 2, batch 30, current loss 0.0677664240822196\n",
      "epoch 2, batch 40, current loss 0.07413392476737499\n",
      "epoch 2, batch 50, current loss 0.06851201839745044\n",
      "epoch 2, batch 60, current loss 0.05189779419451952\n",
      "epoch 2, batch 70, current loss 0.04238828420639038\n",
      "epoch 2, batch 80, current loss 0.04740224294364452\n",
      "epoch 2, batch 90, current loss 0.0472325062379241\n",
      "epoch 2, batch 100, current loss 0.07072065807878972\n",
      "epoch 2, batch 110, current loss 0.03938809903338551\n",
      "epoch 2, batch 120, current loss 0.0419163029640913\n",
      "epoch 2, batch 130, current loss 0.04108637217432261\n",
      "epoch 2, batch 140, current loss 0.06806634087115526\n",
      "epoch 2, batch 150, current loss 0.06166150663048029\n",
      "epoch 2, batch 160, current loss 0.058042684197425844\n",
      "epoch 2, batch 170, current loss 0.06612581815570592\n",
      "epoch 2, batch 180, current loss 0.050076594948768614\n",
      "epoch 2, batch 190, current loss 0.04351442940533161\n",
      "epoch 2, batch 200, current loss 0.051602044701576234\n",
      "epoch 2, batch 210, current loss 0.06339977798052132\n",
      "epoch 2, batch 220, current loss 0.06508688740432263\n",
      "epoch 2, batch 230, current loss 0.04007317209616303\n",
      "epoch 2, batch 240, current loss 0.05313297808170318\n",
      "epoch 2, batch 250, current loss 0.049933566618710755\n",
      "epoch 2, batch 260, current loss 0.03208293924108148\n",
      "epoch 2, batch 270, current loss 0.04243089575320482\n",
      "epoch 2, batch 280, current loss 0.046401013340801\n",
      "epoch 2, batch 290, current loss 0.04113989192992449\n",
      "epoch 2, batch 300, current loss 0.0431053576990962\n",
      "epoch 2, batch 310, current loss 0.038034988287836316\n",
      "epoch 2, batch 320, current loss 0.038555474020540714\n",
      "epoch 2, batch 330, current loss 0.03589450381696224\n",
      "epoch 2, batch 340, current loss 0.02266184762120247\n",
      "epoch 2, batch 350, current loss 0.02340556597337127\n",
      "epoch 2, batch 360, current loss 0.03173035029321909\n",
      "epoch 2, batch 370, current loss 0.04623358016833663\n",
      "epoch 2, batch 380, current loss 0.029762668162584306\n",
      "epoch 2, batch 390, current loss 0.03560230247676373\n",
      "epoch 2, batch 400, current loss 0.04056764736305922\n",
      "epoch 2, batch 410, current loss 0.0401681742630899\n",
      "epoch 2, batch 420, current loss 0.025762511137872934\n",
      "epoch 2, batch 430, current loss 0.026068877801299094\n",
      "epoch 2, batch 440, current loss 0.03644945090636611\n",
      "epoch 2, batch 450, current loss 0.023315486684441566\n",
      "epoch 2, batch 460, current loss 0.0349789772182703\n",
      "epoch 2, batch 470, current loss 0.019767591916024684\n",
      "epoch 2, batch 480, current loss 0.026060248631983994\n",
      "epoch 2, batch 490, current loss 0.012699795234948397\n",
      "epoch 2, batch 500, current loss 0.030308803543448448\n",
      "epoch 2, batch 510, current loss 0.015666228160262106\n",
      "epoch 2, batch 520, current loss 0.01570794526487589\n",
      "epoch 2, batch 530, current loss 0.029000567412003873\n",
      "epoch 2, batch 540, current loss 0.028853822080418468\n",
      "epoch 2, batch 550, current loss 0.012155540846288204\n",
      "epoch 2, batch 560, current loss 0.03303712364286184\n",
      "epoch 2, batch 570, current loss 0.020858581643551588\n",
      "epoch 2, batch 580, current loss 0.019066132907755672\n",
      "epoch 2, batch 590, current loss 0.025902052875608206\n",
      "epoch 2, batch 600, current loss 0.02562149935401976\n",
      "epoch 2, batch 610, current loss 0.02112546213902533\n",
      "epoch 2, batch 620, current loss 0.018745282385498285\n",
      "epoch 2, batch 630, current loss 0.024817949277348816\n",
      "epoch 2, batch 640, current loss 0.02053248672746122\n",
      "epoch 2, batch 650, current loss 0.024389854958280922\n",
      "epoch 2, batch 660, current loss 0.022657874133437873\n",
      "epoch 2, batch 670, current loss 0.024703634064644577\n",
      "epoch 2, batch 680, current loss 0.023354700533673167\n",
      "epoch 2, batch 690, current loss 0.017423425475135444\n",
      "epoch 3, batch 0, current loss 0.005481554940342903\n",
      "epoch 3, batch 10, current loss 0.029028354818001388\n",
      "epoch 3, batch 20, current loss 0.01593622574582696\n",
      "epoch 3, batch 30, current loss 0.031184820318594576\n",
      "epoch 3, batch 40, current loss 0.025754725863225757\n",
      "epoch 3, batch 50, current loss 0.011015723436139524\n",
      "epoch 3, batch 60, current loss 0.015387840126641095\n",
      "epoch 3, batch 70, current loss 0.007405884878244251\n",
      "epoch 3, batch 80, current loss 0.020223960885778068\n",
      "epoch 3, batch 90, current loss 0.012668309733271599\n",
      "epoch 3, batch 100, current loss 0.008573212788905948\n",
      "epoch 3, batch 110, current loss 0.008999948087148369\n",
      "epoch 3, batch 120, current loss 0.016938576591201127\n",
      "epoch 3, batch 130, current loss 0.0072932078037410975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 140, current loss 0.009507321508135647\n",
      "epoch 3, batch 150, current loss 0.016762164968531577\n",
      "epoch 3, batch 160, current loss 0.010308969882316887\n",
      "epoch 3, batch 170, current loss 0.017509105033241212\n",
      "epoch 3, batch 180, current loss 0.012462080549448728\n",
      "epoch 3, batch 190, current loss 0.009122524556005374\n",
      "epoch 3, batch 200, current loss 0.016121569101233035\n",
      "epoch 3, batch 210, current loss 0.0064844349748454985\n",
      "epoch 3, batch 220, current loss 0.014309449295978993\n",
      "epoch 3, batch 230, current loss 0.006916796858422458\n",
      "epoch 3, batch 240, current loss 0.00963301642332226\n",
      "epoch 3, batch 250, current loss 0.007315920165274293\n",
      "epoch 3, batch 260, current loss 0.012476668390445412\n",
      "epoch 3, batch 270, current loss 0.009849526756443083\n",
      "epoch 3, batch 280, current loss 0.008650027180556208\n",
      "epoch 3, batch 290, current loss 0.010360892536118627\n",
      "epoch 3, batch 300, current loss 0.01320838239043951\n",
      "epoch 3, batch 310, current loss 0.007286943926010281\n",
      "epoch 3, batch 320, current loss 0.012701869220472872\n",
      "epoch 3, batch 330, current loss 0.012343556713312864\n",
      "epoch 3, batch 340, current loss 0.010093347262591123\n",
      "epoch 3, batch 350, current loss 0.007254542334703728\n",
      "epoch 3, batch 360, current loss 0.012921316066058352\n",
      "epoch 3, batch 370, current loss 0.0063806027872487904\n",
      "epoch 3, batch 380, current loss 0.008997316926252098\n",
      "epoch 3, batch 390, current loss 0.012931618781294674\n",
      "epoch 3, batch 400, current loss 0.012067736778408288\n",
      "epoch 3, batch 410, current loss 0.014164685376454145\n",
      "epoch 3, batch 420, current loss 0.005928332894109189\n",
      "epoch 3, batch 430, current loss 0.008329066191799938\n",
      "epoch 3, batch 440, current loss 0.008015092159621418\n",
      "epoch 3, batch 450, current loss 0.004620956827420741\n",
      "epoch 3, batch 460, current loss 0.010599416471086443\n",
      "epoch 3, batch 470, current loss 0.01289218373131007\n",
      "epoch 3, batch 480, current loss 0.002589068282395601\n",
      "epoch 3, batch 490, current loss 0.008366526267491282\n",
      "epoch 3, batch 500, current loss 0.004559651960153133\n",
      "epoch 3, batch 510, current loss 0.00984396068379283\n",
      "epoch 3, batch 520, current loss 0.008010668971110135\n",
      "epoch 3, batch 530, current loss 0.013848377903923393\n",
      "epoch 3, batch 540, current loss 0.005478965980000794\n",
      "epoch 3, batch 550, current loss 0.00698065081378445\n",
      "epoch 3, batch 560, current loss 0.011212379357311874\n",
      "epoch 3, batch 570, current loss 0.015399878402240575\n",
      "epoch 3, batch 580, current loss 0.007013457850553096\n",
      "epoch 3, batch 590, current loss 0.00386246326379478\n",
      "epoch 3, batch 600, current loss 0.00341494579333812\n",
      "epoch 3, batch 610, current loss 0.008841304969973863\n",
      "epoch 3, batch 620, current loss 0.005587738880421966\n",
      "epoch 3, batch 630, current loss 0.01101730765076354\n",
      "epoch 3, batch 640, current loss 0.0066085902333725246\n",
      "epoch 3, batch 650, current loss 0.005560651677660644\n",
      "epoch 3, batch 660, current loss 0.005300637788604945\n",
      "epoch 3, batch 670, current loss 0.015523903630673885\n",
      "epoch 3, batch 680, current loss 0.005586205516010523\n",
      "epoch 3, batch 690, current loss 0.006071855284972116\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_net()\n",
    "#net.apply(weight_init)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "print(\"There are {} parameters in the model\".format(count_parameters(net)))\n",
    "\n",
    "num_epochs = 3\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.5,1.0]).to(device))\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.00001)\n",
    "print('choose SGD as optimizer')\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr*10, weight_decay=0.00001)\n",
    "#print('choose Adam as optimizer')\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    idx1=np.random.choice(range(len(train_idx)),89600,replace=False)\n",
    "    train_data_shuffle =[train_idx[idx1[i]] for i in range(len(idx1))]\n",
    "    Mouse_dataset = Mouse_sub_volumes(all_train_data,train_data_shuffle,\n",
    "                                     transform=transforms.Compose([Rotate(),Flip()]))\n",
    "    dataloader = DataLoader(Mouse_dataset, batch_size=128,\n",
    "                        shuffle=True, num_workers=4)\n",
    "    train(net, device, dataloader, optimizer, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236874\n",
      "42962\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx))\n",
    "print(len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# net = VGG_net()\n",
    "# #net.apply(weight_init)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     net = nn.DataParallel(net)\n",
    "# net.to(device)\n",
    "# print(\"There are {} parameters in the model\".format(count_parameters(net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "            inputs = inputs.to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    \n",
    "\n",
    "def test2(model, device, test_loader, test_img_list):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    test_dic = {}\n",
    "    for idx in test_img_list:\n",
    "        test_dic[idx] = [0,0]\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels, current_img = sample_batched['image'], sample_batched['label'], sample_batched['img_idx']    \n",
    "            inputs = inputs.to(device)\n",
    "            current_img = current_img.numpy()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(len(labels)):\n",
    "                test_dic[current_img[i]][predicted[i].cpu().numpy()] +=1\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    return test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), './model/mut_clas_e1_v0105_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load('./model/mut_clas_e1_v1107.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train accuracy: ')\n",
    "# Mouse_dataset = Mouse_sub_volumes(all_train_data,train_idx)\n",
    "# train_dataloader = DataLoader(Mouse_dataset, batch_size=128,\n",
    "#                         shuffle=False, num_workers=4)\n",
    "# test(net, device, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: \n",
      "total_num:42962, test accuracy:0.8501466412178204, positive_acc:0.9205470257571944, negative_acc:0.7623928048525414\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy: ')\n",
    "Mouse_dataset = Mouse_sub_volumes(all_train_data,test_idx)\n",
    "test_dataloader = DataLoader(Mouse_dataset, batch_size=128,\n",
    "                        shuffle=False, num_workers=4)\n",
    "test_dic = test2(net, device, test_dataloader, test_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{255: ['pred_as_mul', 'true_as_mul'], 256: ['pred_as_mul', 'true_as_mul'], 257: ['pred_as_mul', 'true_as_mul'], 258: ['pred_as_nor', 'true_as_mul'], 268: ['pred_as_mul', 'true_as_mul'], 290: ['pred_as_mul', 'true_as_mul'], 293: ['pred_as_mul', 'true_as_mul'], 294: ['pred_as_mul', 'true_as_mul'], 300: ['pred_as_mul', 'true_as_mul'], 304: ['pred_as_nor', 'true_as_mul'], 305: ['pred_as_nor', 'true_as_mul'], 306: ['pred_as_mul', 'true_as_mul'], 307: ['pred_as_nor', 'true_as_mul'], 312: ['pred_as_mul', 'true_as_mul'], 316: ['pred_as_mul', 'true_as_mul'], 259: ['pred_as_nor', 'true_as_nor'], 260: ['pred_as_nor', 'true_as_nor'], 261: ['pred_as_nor', 'true_as_nor'], 262: ['pred_as_nor', 'true_as_nor'], 263: ['pred_as_nor', 'true_as_nor'], 264: ['pred_as_nor', 'true_as_nor'], 265: ['pred_as_nor', 'true_as_nor'], 266: ['pred_as_nor', 'true_as_nor'], 267: ['pred_as_nor', 'true_as_nor'], 269: ['pred_as_nor', 'true_as_nor'], 270: ['pred_as_nor', 'true_as_nor'], 271: ['pred_as_nor', 'true_as_nor'], 272: ['pred_as_nor', 'true_as_nor'], 273: ['pred_as_nor', 'true_as_nor'], 274: ['pred_as_nor', 'true_as_nor'], 275: ['pred_as_nor', 'true_as_nor'], 276: ['pred_as_nor', 'true_as_nor'], 277: ['pred_as_nor', 'true_as_nor'], 278: ['pred_as_nor', 'true_as_nor'], 279: ['pred_as_nor', 'true_as_nor'], 280: ['pred_as_nor', 'true_as_nor'], 281: ['pred_as_nor', 'true_as_nor'], 282: ['pred_as_nor', 'true_as_nor'], 283: ['pred_as_nor', 'true_as_nor'], 284: ['pred_as_nor', 'true_as_nor'], 285: ['pred_as_nor', 'true_as_nor'], 286: ['pred_as_nor', 'true_as_nor'], 287: ['pred_as_mul', 'true_as_nor'], 288: ['pred_as_nor', 'true_as_nor'], 289: ['pred_as_nor', 'true_as_nor'], 291: ['pred_as_nor', 'true_as_nor'], 292: ['pred_as_nor', 'true_as_nor'], 295: ['pred_as_nor', 'true_as_nor'], 296: ['pred_as_mul', 'true_as_nor'], 297: ['pred_as_nor', 'true_as_nor'], 298: ['pred_as_nor', 'true_as_nor'], 299: ['pred_as_nor', 'true_as_nor'], 301: ['pred_as_nor', 'true_as_nor'], 302: ['pred_as_nor', 'true_as_nor'], 303: ['pred_as_nor', 'true_as_nor'], 308: ['pred_as_nor', 'true_as_nor'], 309: ['pred_as_nor', 'true_as_nor'], 310: ['pred_as_nor', 'true_as_nor'], 311: ['pred_as_nor', 'true_as_nor'], 313: ['pred_as_nor', 'true_as_nor'], 314: ['pred_as_nor', 'true_as_nor'], 315: ['pred_as_nor', 'true_as_nor'], 317: ['pred_as_nor', 'true_as_nor'], 318: ['pred_as_nor', 'true_as_nor'], 319: ['pred_as_nor', 'true_as_nor'], 320: ['pred_as_nor', 'true_as_nor'], 321: ['pred_as_nor', 'true_as_nor'], 322: ['pred_as_nor', 'true_as_nor'], 323: ['pred_as_nor', 'true_as_nor'], 324: ['pred_as_nor', 'true_as_nor'], 325: ['pred_as_nor', 'true_as_nor'], 326: ['pred_as_nor', 'true_as_nor'], 327: ['pred_as_nor', 'true_as_nor'], 328: ['pred_as_nor', 'true_as_nor'], 329: ['pred_as_nor', 'true_as_nor'], 330: ['pred_as_nor', 'true_as_nor'], 331: ['pred_as_nor', 'true_as_nor'], 332: ['pred_as_nor', 'true_as_nor'], 335: ['pred_as_nor', 'true_as_nor'], 337: ['pred_as_nor', 'true_as_nor'], 338: ['pred_as_mul', 'true_as_nor'], 339: ['pred_as_nor', 'true_as_nor'], 342: ['pred_as_nor', 'true_as_nor'], 343: ['pred_as_nor', 'true_as_nor'], 344: ['pred_as_nor', 'true_as_nor'], 345: ['pred_as_nor', 'true_as_nor'], 346: ['pred_as_nor', 'true_as_nor'], 347: ['pred_as_nor', 'true_as_nor'], 348: ['pred_as_nor', 'true_as_nor'], 349: ['pred_as_nor', 'true_as_nor'], 350: ['pred_as_nor', 'true_as_nor'], 351: ['pred_as_nor', 'true_as_nor'], 353: ['pred_as_nor', 'true_as_nor'], 354: ['pred_as_mul', 'true_as_nor'], 355: ['pred_as_nor', 'true_as_nor'], 364: ['pred_as_nor', 'true_as_nor'], 367: ['pred_as_nor', 'true_as_nor'], 368: ['pred_as_nor', 'true_as_nor'], 369: ['pred_as_mul', 'true_as_nor']}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "test_dic2 = copy.deepcopy(test_dic)\n",
    "for key in test_dic:\n",
    "    if test_dic[key][0] > test_dic[key][1]:\n",
    "        test_dic2[key][0] = 'pred_as_mul'\n",
    "    else:\n",
    "        test_dic2[key][0] = 'pred_as_nor'\n",
    "    if key in test_mutant_list:\n",
    "        test_dic2[key][1] = 'true_as_mul'\n",
    "    elif key in test_normal_list:\n",
    "        test_dic2[key][1] = 'true_as_nor'\n",
    "print(test_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{255: 1.0, 256: 1.0, 257: 1.0, 258: 0.0, 268: 0.9921197793538219, 290: 0.5047619047619047, 293: 1.0, 294: 1.0, 300: 1.0, 304: 0.01542111506524318, 305: 0.0042780748663101605, 306: 0.9451058201058201, 307: 0.23380281690140844, 312: 0.8044382801664355, 316: 0.9682966147232671, 259: 0.0, 260: 0.0, 261: 0.0, 262: 0.0, 263: 0.0, 264: 0.0, 265: 0.0, 266: 0.0, 267: 0.0, 269: 0.0, 270: 0.0, 271: 0.0, 272: 0.0, 273: 0.0, 274: 0.0, 275: 0.029166666666666667, 276: 0.0, 277: 0.0, 278: 0.050347222222222224, 279: 0.0, 280: 0.0, 281: 0.0, 282: 0.0, 283: 0.0, 284: 0.0, 285: 0.0, 286: 0.04377104377104377, 287: 0.9888888888888889, 288: 0.0, 289: 0.0, 291: 0.03676470588235294, 292: 0.004273504273504274, 295: 0.041666666666666664, 296: 1.0, 297: 0.003787878787878788, 298: 0.0, 299: 0.0, 301: 0.24206349206349206, 302: 0.0, 303: 0.0, 308: 0.0, 309: 0.0, 310: 0.0, 311: 0.007407407407407408, 313: 0.0, 314: 0.4722222222222222, 315: 0.35785953177257523, 317: 0.0, 318: 0.3619047619047619, 319: 0.0, 320: 0.0, 321: 0.0, 322: 0.0, 323: 0.0, 324: 0.0, 325: 0.0, 326: 0.0, 327: 0.0, 328: 0.1380952380952381, 329: 0.0, 330: 0.0, 331: 0.0, 332: 0.0, 335: 0.0, 337: 0.0, 338: 0.8194444444444444, 339: 0.0, 342: 0.0, 343: 0.0, 344: 0.0873015873015873, 345: 0.0, 346: 0.0, 347: 0.0, 348: 0.0, 349: 0.0, 350: 0.0, 351: 0.0, 353: 0.0, 354: 0.6507936507936508, 355: 0.0, 364: 0.4027777777777778, 367: 0.0, 368: 0.0, 369: 0.9417989417989417}\n"
     ]
    }
   ],
   "source": [
    "test_dic3 = {}\n",
    "for key in test_dic:\n",
    "        test_dic3[key] = test_dic[key][0]/(test_dic[key][0]+test_dic[key][1])\n",
    "print(test_dic3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.]\n",
      " [  5.  79.]]\n",
      "[258, 304, 305, 307]\n",
      "[287, 296, 338, 354, 369]\n"
     ]
    }
   ],
   "source": [
    "cross_table = np.zeros([2,2])\n",
    "mut_to_nor = []\n",
    "nor_to_mul = []\n",
    "thre = 0.5\n",
    "for key in test_dic:\n",
    "    if test_dic3[key] >= thre and key in test_mutant_list:\n",
    "        cross_table[0,0] += 1\n",
    "    elif  test_dic3[key] < thre and key in test_mutant_list:\n",
    "        cross_table[0,1] += 1\n",
    "        mut_to_nor.append(key)\n",
    "    elif test_dic3[key] >= thre and key in test_normal_list:\n",
    "        cross_table[1,0] += 1\n",
    "        nor_to_mul.append(key)\n",
    "    elif test_dic3[key] < thre and key in test_normal_list:\n",
    "        cross_table[1,1] += 1\n",
    "print(cross_table)\n",
    "print(mut_to_nor)\n",
    "print(nor_to_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mut_to_nor:\n",
    "    print(i, ': ', test_dic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nor_to_mul:\n",
    "    print(i, ': ', test_dic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mut_to_nor:\n",
    "    print(i)\n",
    "    img_nft = nib.Nifti1Image(np.squeeze(all_train_data[i]),np.eye(4))\n",
    "    img_save_data_path = './img/mul_img{}.nii'.format(i)\n",
    "    nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nor_to_mul:\n",
    "    print(i)\n",
    "    img_nft = nib.Nifti1Image(np.squeeze(all_train_data[i]),np.eye(4))\n",
    "    img_save_data_path = './img/mul_img{}.nii'.format(i)\n",
    "    nib.save(img_nft,img_save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(model, device, test_loader, test_img_list):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    test_dic = {}\n",
    "    for idx in test_img_list:\n",
    "        test_dic[idx] = [0,0]\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels, current_img = sample_batched['image'], sample_batched['label'], sample_batched['img_idx']    \n",
    "            inputs = inputs.to(device)\n",
    "            current_img = current_img.numpy()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(len(labels)):\n",
    "                test_dic[current_img[i]][predicted[i].cpu().numpy()] +=1\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "    return test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cross_table, index=['multant','normal'],columns=['pred as mul','pred as nor'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
